# Knowledge Graph Construction

This chapter explores the process of constructing knowledge graphs from various data sources. We'll examine approaches for extracting entities and relationships, integrating heterogeneous information, and ensuring the quality and coherence of the resulting knowledge graph.

## Knowledge graph construction lifecycle

Knowledge graph construction involves a systematic process of extracting, transforming, and integrating knowledge from diverse sources into a coherent graph-structured representation.

### Overview of construction approaches

::: {#def-kg-construction}

## Knowledge graph construction

**Knowledge graph construction** is the process of creating a knowledge graph by:

1. Extracting entities, relationships, and attributes from various data sources
2. Aligning and integrating the extracted elements into a unified schema
3. Ensuring consistency, quality, and coherence of the resulting graph

Construction approaches can be categorized as:

1. **Top-down**: Starting with an ontology/schema and populating it with instances
2. **Bottom-up**: Extracting knowledge first and then organizing it into a schema
3. **Hybrid**: Combining elements of both approaches

:::

::: {#exm-construction-approaches}

## Knowledge graph construction approaches

1. **Top-down approach**:

   - Create an ontology defining classes (Person, Organization, Location) and relationships (employedBy, locatedIn)
   - Define mapping rules from source data to ontology concepts
   - Extract and transform data according to these mappings
   - Example: Building an enterprise knowledge graph with a predefined schema based on organizational data models

2. **Bottom-up approach**:

   - Extract entities and relationships from text, databases, or other sources
   - Cluster similar entities and relationships
   - Induce a schema or ontology from the patterns in the data
   - Example: Creating an open-domain knowledge graph from web text without a predefined schema

3. **Hybrid approach**:
   - Start with a lightweight core schema defining basic types and relations
   - Extract entities and relationships, mapping to schema where possible
   - Extend schema based on discovered patterns
   - Iteratively refine both schema and instance data
   - Example: Building a scientific knowledge graph with a core schema of established concepts, but allowing for expansion as new research areas emerge

:::

The choice of approach depends on factors such as:

- Availability of existing schemas or ontologies
- Nature and structure of source data
- Requirements for flexibility versus consistency
- Domain expertise and resources available

### Construction lifecycle phases

The knowledge graph construction process typically follows a lifecycle with distinct phases:

::: {#def-kg-lifecycle}

## Knowledge graph construction lifecycle

The **knowledge graph construction lifecycle** typically includes the following phases:

1. **Planning and design**: Defining scope, requirements, and conceptual model
2. **Schema development**: Creating or selecting ontologies and schemas
3. **Data acquisition**: Gathering relevant source data
4. **Information extraction**: Identifying entities, relationships, and attributes
5. **Integration and fusion**: Combining and reconciling information from multiple sources
6. **Quality assessment**: Evaluating and improving the knowledge graph
7. **Enrichment**: Adding additional knowledge through inference or external sources
8. **Deployment and maintenance**: Making the knowledge graph available and keeping it updated

:::

::: {#exm-kg-lifecycle}

## Knowledge graph construction lifecycle example

**Project**: Building a biomedical knowledge graph for drug discovery

1. **Planning and design**:

   - Define scope: Focus on diseases, drugs, targets, and biological pathways
   - Identify requirements: Support for finding potential drug repurposing opportunities
   - Create conceptual model: Entity types, relationship types, and their connections

2. **Schema development**:

   - Adopt existing ontologies: Disease Ontology, ChEBI (for chemicals), Gene Ontology
   - Develop custom extensions for project-specific concepts
   - Define mappings between different ontologies

3. **Data acquisition**:

   - Identify sources: PubMed (literature), DrugBank (drugs), KEGG (pathways)
   - Establish access methods: APIs, downloads, web scraping
   - Set up preprocessing pipelines for each source

4. **Information extraction**:

   - Extract drug-disease relationships from literature using NLP
   - Parse structured databases to extract drug-target interactions
   - Identify gene-pathway relationships from pathway databases

5. **Integration and fusion**:

   - Resolve entity references across sources (entity resolution)
   - Reconcile conflicting information about relationships
   - Harmonize relationship types from different sources

6. **Quality assessment**:

   - Verify extracted relationships against gold standard datasets
   - Calculate precision, recall, and coverage metrics
   - Identify and address systematic errors

7. **Enrichment**:

   - Infer new drug-disease relationships based on shared mechanisms
   - Add confidence scores to relationships based on evidence strength
   - Link to external resources for additional context

8. **Deployment and maintenance**:
   - Deploy as a queryable graph database with API access
   - Establish update schedule for incorporating new publications
   - Monitor usage patterns to guide further development

:::

These phases often occur iteratively rather than strictly sequentially, with feedback from later phases informing refinements to earlier phases. For example, quality assessment might reveal the need for additional information extraction techniques, or deployment experiences might suggest schema modifications.

### Construction strategies and architectures

Various architectural approaches can be used to implement knowledge graph construction:

::: {#def-kg-architectures}

## Knowledge graph construction architectures

**Knowledge graph construction architectures** define the systems, components, and data flows for building and maintaining knowledge graphs:

1. **Batch processing architecture**: Processing large volumes of source data in scheduled batches
2. **Streaming architecture**: Continuously processing incoming data in near real-time
3. **Hybrid architecture**: Combining batch processing for historical data with streaming for updates
4. **Federated architecture**: Leaving data in source systems and constructing a virtual knowledge graph via query translation

:::

::: {#exm-kg-architectures}

## Knowledge graph construction architectures

1. **Batch processing architecture**:

   - Components: Data lake storage, extraction workflows, transformation jobs, graph database loader
   - Process: Weekly full extraction from sources → parallel processing pipelines → quality checks → database update
   - Tools: Apache Hadoop, Spark, orchestration with Airflow
   - Example use case: Academic knowledge graph updated quarterly with new publications

2. **Streaming architecture**:

   - Components: Message brokers, stream processors, incremental extractors, graph database with transactional updates
   - Process: Continuous ingestion of events → real-time entity extraction → immediate graph updates
   - Tools: Kafka, Flink, streaming NLP services
   - Example use case: Financial knowledge graph reflecting market changes in near real-time

3. **Hybrid architecture**:

   - Components: Batch pipeline for historical data, streaming pipeline for updates, reconciliation mechanisms
   - Process: Initial bulk load via batch processes → continuous updates via streaming → periodic reprocessing of historical data
   - Tools: Combined Spark (batch) and Kafka/Flink (streaming) infrastructure
   - Example use case: E-commerce knowledge graph with historical catalog data and real-time inventory updates

4. **Federated architecture**:
   - Components: Source connectors, query federation engine, schema mapping layer, caching mechanisms
   - Process: Query received → decomposed into source-specific queries → results fetched and integrated on-the-fly
   - Tools: Query federation platforms, API gateways, virtualization middleware
   - Example use case: Healthcare knowledge graph integrating multiple clinical systems without centralizing sensitive patient data

:::

The choice of architecture depends on factors such as:

- Update frequency requirements
- Volume and velocity of source data
- Latency requirements for access
- Integration complexity
- Data privacy and governance considerations

## Data sources for knowledge graphs

Knowledge graphs can be constructed from a wide variety of data sources, each requiring different extraction and integration approaches.

### Structured data sources

Structured data sources provide information in well-defined formats with explicit schemas:

::: {#def-structured-sources}

## Structured data sources

**Structured data sources** contain information organized according to a predefined data model. Key types include:

1. **Relational databases**: Data organized in tables with relationships defined by keys
2. **XML/JSON documents**: Hierarchically structured data with defined schemas
3. **RDF stores**: Graph data already expressed in subject-predicate-object triples
4. **CSV/tabular data**: Data organized in rows and columns with defined semantics
5. **APIs with structured responses**: Web services returning data in structured formats

:::

::: {#exm-structured-sources}

## Structured data integration examples

1. **Relational database integration**:

   - Example: Converting a customer database to a knowledge graph
   - Approach:
     - Tables → entity classes (Customers, Products, Orders)
     - Rows → entity instances
     - Foreign keys → relationships
     - Columns → attributes
   - Mapping example:
     ```
     CREATE MAPPING CompanyMap FOR Customer {
       uri: template("company/{id}");
       types: ["ex:Company"];
       properties: [
         property(rdfs:label, name),
         property(ex:foundedIn, founded_year),
         link(ex:hasIndustry, industry_code, "industry/{value}")
       ]
     }
     ```

2. **XML integration**:

   - Example: Converting product catalog XML to a knowledge graph
   - Approach:
     - XML elements → entities or attributes
     - Element hierarchies → relationships
     - XML attributes → entity attributes
   - XSLT transformation example:
     ```xml
     <xsl:template match="product">
       <xsl:variable name="productURI" select="concat('product/', @id)"/>
       <!-- Create product entity -->
       <ex:Product rdf:about="{$productURI}">
         <rdfs:label><xsl:value-of select="name"/></rdfs:label>
         <ex:price><xsl:value-of select="price"/></ex:price>
         <!-- Create relationship to category -->
         <ex:hasCategory rdf:resource="{concat('category/', category/@id)}"/>
       </ex:Product>
     </xsl:template>
     ```

3. **API integration**:
   - Example: Converting GitHub API responses to a knowledge graph
   - Approach:
     - API endpoints → entity types (repositories, users, issues)
     - JSON objects → entity instances
     - Object references → relationships
     - Fields → attributes
   - GraphQL query example:
     ```graphql
     query {
       repository(owner: "tensorflow", name: "tensorflow") {
         name
         description
         stargazerCount
         pullRequests(first: 10, states: OPEN) {
           nodes {
             title
             author {
               login
             }
           }
         }
       }
     }
     ```
     Results mapped to knowledge graph entities and relationships

:::

Integration challenges for structured data include:

1. **Schema heterogeneity**: Differences in how similar concepts are modeled across sources
2. **Semantic mismatches**: Different interpretations of the same terms
3. **Entity resolution**: Identifying when records from different sources refer to the same entity
4. **Data quality issues**: Inconsistent formats, missing values, or contradictory information

### Semi-structured data sources

Semi-structured data sources lack rigid schemas but contain implicit structure:

::: {#def-semi-structured}

## Semi-structured data sources

**Semi-structured data sources** contain information with some organizational patterns but without a rigid, predefined schema. Key types include:

1. **HTML web pages**: Content organized using HTML tags and DOM structure
2. **Wikis and collaborative platforms**: Content with lightweight markup and templates
3. **JSON/XML without strict schemas**: Data with flexible, nested structure
4. **Email and document repositories**: Content with some structural elements but variable organization
5. **Social media content**: Posts, profiles, and interactions with platform-specific structures

:::

::: {#exm-semi-structured}

## Semi-structured data extraction examples

1. **Web page extraction**:

   - Example: Extracting product information from e-commerce websites
   - Approach:
     - Use HTML structure (lists, tables, divs) to identify entity boundaries
     - Use CSS selectors or XPath to target specific information
     - Extract entities and relationships from page content and navigation
   - Web scraping example:
     ```python
     # Extract product details using CSS selectors
     product = {
       'name': page.select_one('h1.product-title').text,
       'price': page.select_one('span.price').text,
       'manufacturer': page.select_one('div.brand a').text,
       'categories': [a.text for a in page.select('ul.breadcrumbs a')],
       'features': [li.text for li in page.select('ul.features li')]
     }
     ```

2. **Wiki extraction**:

   - Example: Building a knowledge graph from Wikipedia
   - Approach:
     - Extract structured data from infoboxes (templated metadata)
     - Use section headers to organize information hierarchically
     - Extract entities and relationships from links and categories
     - Apply NLP to extract additional relationships from text
   - Wikipedia extraction example:
     ```
     # From Wikipedia infobox for Berlin
     Entity: Berlin
     Type: City, Capital
     Country: Germany
     Population: 3,664,088
     Area: 891.7 km²
     Mayor: Franziska Giffey
     ```

3. **Social media extraction**:
   - Example: Building a social network knowledge graph from Twitter
   - Approach:
     - Extract users as primary entities
     - Extract relationships from follows, mentions, replies
     - Extract topics and sentiments from post content
     - Build temporal patterns from posting timestamps
   - Twitter extraction example:
     ```
     # From tweet data structure
     Entity: User123
     Relationship: posted -> Tweet456
     Relationship: mentioned -> User789 in Tweet456
     Relationship: hashtagged -> #AI in Tweet456
     Property: Tweet456.sentiment = positive
     Property: Tweet456.timestamp = 2023-05-15T14:30:00Z
     ```

:::

Integration challenges for semi-structured data include:

1. **Inconsistent formatting**: Variations in how information is presented
2. **Implicit relationships**: Connections that are implied rather than explicitly stated
3. **Noise and irrelevant content**: Filtering out elements that don't contribute to the knowledge graph
4. **Structure recognition**: Identifying meaningful patterns in variable layouts

### Unstructured data sources

Unstructured data sources contain information without explicit organization:

::: {#def-unstructured}

## Unstructured data sources

**Unstructured data sources** contain information with no predefined data model or organized structure. Key types include:

1. **Natural language text**: Articles, reports, books, or other written documents
2. **Audio recordings**: Spoken conversations, lectures, or interviews
3. **Images**: Photographs, diagrams, charts, or other visual content
4. **Videos**: Visual and auditory content with temporal dimension
5. **Scientific data**: Experimental results, observations, measurements

:::

::: {#exm-unstructured}

## Unstructured data extraction examples

1. **Natural language text extraction**:

   - Example: Building a knowledge graph from scientific papers
   - Approach:
     - Apply named entity recognition to identify key entities
     - Use relationship extraction to identify connections between entities
     - Apply coreference resolution to connect mentions of the same entity
     - Extract numerical values and measurements as attributes
   - Text extraction example:

     ```
     Input text: "BERT was developed by Google and was first published in 2018."

     Extracted:
     Entity: BERT (type: Model)
     Entity: Google (type: Organization)
     Relationship: developedBy(BERT, Google)
     Attribute: publishedYear(BERT, 2018)
     ```

2. **Image extraction**:

   - Example: Building a knowledge graph from medical images
   - Approach:
     - Apply object detection to identify anatomical structures
     - Use image segmentation to delineate regions of interest
     - Extract relationships based on spatial arrangement
     - Classify abnormalities and link to condition entities
   - Image extraction example:

     ```
     Input: Chest X-ray image

     Extracted:
     Entity: LeftLung_PatientX (type: LeftLung)
     Entity: RightLung_PatientX (type: RightLung)
     Entity: Nodule123 (type: PulmonaryNodule)
     Relationship: locatedIn(Nodule123, RightLung_PatientX)
     Attribute: diameter(Nodule123, "12mm")
     ```

3. **Audio extraction**:
   - Example: Building a knowledge graph from meeting recordings
   - Approach:
     - Apply speech recognition to convert audio to text
     - Use speaker diarization to identify who said what
     - Apply NLP to the resulting text to extract entities and relationships
     - Extract temporal relationships from sequence of topics
   - Audio extraction example:

     ```
     Input: Meeting recording

     Extracted:
     Entity: Meeting_20230520 (type: Meeting)
     Entity: ProjectAlpha (type: Project)
     Relationship: discussed(Meeting_20230520, ProjectAlpha)
     Relationship: participated(JohnDoe, Meeting_20230520)
     Relationship: stated(JohnDoe, "We need to increase the budget")
     ```

:::

Integration challenges for unstructured data include:

1. **Information extraction accuracy**: Errors in identifying entities and relationships
2. **Ambiguity resolution**: Determining the correct interpretation of ambiguous terms
3. **Incompleteness**: Missing context that would be obvious to human readers
4. **Cross-modal integration**: Combining insights from different types of unstructured data

### Combining multiple source types

Real-world knowledge graphs typically integrate information from multiple source types:

::: {#def-multi-source}

## Multi-source integration

**Multi-source integration** involves combining data from different source types (structured, semi-structured, and unstructured) into a unified knowledge graph. Key aspects include:

1. **Source prioritization**: Determining which sources to trust when information conflicts
2. **Information alignment**: Mapping concepts and entities across different source types
3. **Complementary extraction**: Using different sources to fill different aspects of the knowledge graph
4. **Cross-validation**: Verifying information by checking consistency across sources
5. **Confidence scoring**: Assigning reliability metrics based on source characteristics

:::

::: {#exm-multi-source}

## Multi-source integration example

**Project**: Building a product knowledge graph

**Sources**:

1. **Structured**: Product database (SKUs, specifications, pricing)
2. **Semi-structured**: Product web pages (features, compatibility, images)
3. **Unstructured**: Customer reviews (experiences, issues, comparisons)

**Integration approach**:

1. **Core entity creation from structured data**:

   - Create product entities with basic attributes from database
   - Example: Product123 (category: Laptop, price: $999, weight: 1.2kg)

2. **Enrichment from semi-structured data**:

   - Add detailed features and relationships from product pages
   - Example: compatibleWith(Product123, Accessory456), hasFeature(Product123, "10-hour battery life")

3. **Augmentation from unstructured data**:

   - Extract sentiment, common issues, and comparisons from reviews
   - Example: commonIssue(Product123, "Overheating"), comparedFavorablyTo(Product123, Competitor789)

4. **Conflict resolution**:

   - For basic specifications: trust database over web pages
   - For user experiences: aggregate from multiple reviews with confidence scoring
   - For temporal information (e.g., price changes): maintain provenance and timestamps

5. **Result**: A comprehensive knowledge graph with:
   - Technical specifications (from structured data)
   - Feature details and compatibility (from semi-structured data)
   - User experiences and comparative positioning (from unstructured data)
   - Confidence scores and provenance for each piece of information

:::

Best practices for multi-source integration include:

1. **Maintaining provenance**: Tracking which source provided each piece of information
2. **Implementing conflict resolution policies**: Defining rules for handling contradictory information
3. **Developing unified entity representations**: Creating consistent entity references across sources
4. **Employing incremental integration**: Building the graph progressively as new sources are processed
5. **Supporting temporal versioning**: Tracking how information changes over time across sources

## Information extraction techniques

Information extraction is the process of identifying and extracting structured information from unstructured or semi-structured sources. This section covers key techniques for extracting entities, relationships, and attributes for knowledge graphs.

### Named entity recognition

Named entity recognition (NER) identifies and classifies named entities in text:

::: {#def-ner}

## Named entity recognition

**Named entity recognition (NER)** is the task of identifying mentions of named entities in text and classifying them into predefined categories such as:

1. **Persons**: Individual people (e.g., Albert Einstein, Marie Curie)
2. **Organizations**: Companies, institutions, agencies (e.g., Google, WHO, MIT)
3. **Locations**: Physical places (e.g., Paris, Mount Everest, Lake Michigan)
4. **Products**: Commercial products (e.g., iPhone, Toyota Camry)
5. **Dates and times**: Temporal expressions (e.g., May 2023, next Tuesday)
6. **Numeric expressions**: Quantities, percentages, monetary values

Domain-specific NER may include additional categories relevant to particular fields (e.g., proteins, diseases, legal citations).

:::

::: {#exm-ner}

## Named entity recognition examples

1. **General-domain NER**:

   ```
   Input: "Apple is planning to open a new campus in Austin, Texas by 2025."

   Output:
   [Apple](Organization) is planning to open a new campus in [Austin, Texas](Location) by [2025](Time).
   ```

2. **Biomedical NER**:

   ```
   Input: "Patients with diabetes mellitus often develop hypertension and may require treatment with metformin."

   Output:
   Patients with [diabetes mellitus](Disease) often develop [hypertension](Disease) and may require treatment with [metformin](Drug).
   ```

3. **Financial NER**:

   ```
   Input: "Tesla reported a quarterly revenue of $24.9 billion, exceeding Wall Street expectations."

   Output:
   [Tesla](Organization) reported a quarterly revenue of [$24.9 billion](Money), exceeding [Wall Street](Organization) expectations.
   ```

:::

NER approaches include:

1. **Rule-based methods**:

   - Using gazetteers (lists of known entities)
   - Pattern matching with regular expressions
   - Grammatical rules specific to entity types
   - Advantages: Interpretable, can work with limited training data
   - Disadvantages: Limited coverage, labor-intensive to create rules

2. **Machine learning methods**:

   - Traditional: Conditional Random Fields (CRF), Support Vector Machines (SVM)
   - Deep learning: Bi-directional LSTMs, Transformer-based models (BERT, RoBERTa)
   - Advantages: Better generalization, can learn from data
   - Disadvantages: Require substantial training data, may lack interpretability

3. **Hybrid approaches**:
   - Combining rule-based and machine learning methods
   - Using rules to augment training data
   - Using machine learning with rule-based features
   - Advantages: Combines strengths of both approaches
   - Disadvantages: More complex to implement and maintain

::: {#exm-ner-implementation}

## NER implementation approaches

1. **Rule-based NER implementation**:

   ```python
   # Simplified rule-based NER example
   import re

   # Patterns for different entity types
   patterns = {
       'date': r'\d{1,2}/\d{1,2}/\d{4}|\d{4}-\d{2}-\d{2}',
       'email': r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
       'url': r'https?://[^\s]+'
   }

   # Apply patterns to text
   def rule_based_ner(text):
       entities = []
       for entity_type, pattern in patterns.items():
           for match in re.finditer(pattern, text):
               entities.append({
                   'text': match.group(),
                   'type': entity_type,
                   'start': match.start(),
                   'end': match.end()
               })
       return entities
   ```

2. **Deep learning NER implementation** (using a library):

   ```python
   # Using spaCy for NER
   import spacy

   # Load pre-trained model
   nlp = spacy.load("en_core_web_lg")

   def ml_based_ner(text):
       doc = nlp(text)
       entities = []
       for ent in doc.ents:
           entities.append({
               'text': ent.text,
               'type': ent.label_,
               'start': ent.start_char,
               'end': ent.end_char
           })
       return entities
   ```

3. **Fine-tuning for domain-specific NER**:

   ```python
   # Example of preparing data for fine-tuning a domain-specific NER model
   train_data = [
       ("Aspirin may interact with warfarin and increase bleeding risk.",
         {"entities": [(0, 7, "DRUG"), (23, 31, "DRUG")]}),
       ("Patients with hypertension should monitor blood pressure regularly.",
         {"entities": [(14, 26, "DISEASE")]})
   ]

   # The actual fine-tuning code would depend on the specific library/framework
   ```

:::

Challenges in NER for knowledge graph construction include:

1. **Entity boundary detection**: Determining where entities start and end
2. **Entity type ambiguity**: Resolving cases where an entity could belong to multiple categories
3. **Context dependence**: Interpreting entities based on surrounding context
4. **Domain adaptation**: Adapting general NER systems to specific domains
5. **Nested entities**: Handling entities that contain other entities

### Relationship extraction

Relationship extraction identifies semantic relationships between entities:

::: {#def-relation-extraction}

## Relationship extraction

**Relationship extraction** is the task of identifying semantic relationships between entities mentioned in text. The process involves:

1. Identifying entity mentions in text (often using NER)
2. Determining which entity pairs might be related
3. Classifying the type of relationship between entities
4. Extracting any additional attributes of the relationship

Relationship types can include:

- Hierarchical (is-a, part-of)
- Action-based (created, affects, causes)
- Social (knows, works-for, married-to)
- Spatial (located-in, near)
- Temporal (precedes, during)
- Domain-specific (treats, inhibits, regulates)

:::

::: {#exm-relation-extraction}

## Relationship extraction examples

1. **General domain**:

   ```
   Input: "Tim Cook, who has been the CEO of Apple since 2011, introduced the new iPhone yesterday."

   Extracted relationships:
   - hasRole(Tim Cook, CEO)
   - employedBy(Tim Cook, Apple)
   - startDate(Tim Cook employed by Apple, 2011)
   - introduced(Tim Cook, iPhone)
   ```

2. **Biomedical domain**:

   ```
   Input: "Metformin reduces glucose production in the liver and improves insulin sensitivity."

   Extracted relationships:
   - reduces(Metformin, glucose production)
   - locatedIn(glucose production, liver)
   - improves(Metformin, insulin sensitivity)
   ```

3. **Financial domain**:

   ```
   Input: "Amazon acquired Whole Foods for $13.7 billion in 2017."

   Extracted relationships:
   - acquired(Amazon, Whole Foods)
   - hasValue(acquisition, $13.7 billion)
   - occurredIn(acquisition, 2017)
   ```

:::

Relationship extraction approaches include:

1. **Pattern-based methods**:

   - Leveraging linguistic patterns (e.g., "X is employed by Y")
   - Using dependency parsing to capture grammatical relationships
   - Applying regular expressions or lexico-syntactic patterns
   - Advantages: Interpretable, precision-oriented
   - Disadvantages: Limited recall, difficult to cover all variations

2. **Supervised learning methods**:

   - Feature-based: Using linguistic features with traditional ML algorithms
   - Neural networks: CNN, RNN, or transformer architectures
   - Distant supervision: Using existing knowledge bases to automatically generate training data
   - Advantages: Better generalization, higher recall
   - Disadvantages: Require substantial training data

3. **Open information extraction**:
   - Extracting relational tuples without pre-defined relation types
   - Identifying subject-predicate-object structures in sentences
   - Advantages: Domain-independent, no predefined schema
   - Disadvantages: Noisy, requires post-processing to normalize relations

::: {#exm-relation-implementation}

## Relationship extraction implementation

1. **Pattern-based extraction**:

   ```python
   import spacy

   # Load spaCy model
   nlp = spacy.load("en_core_web_lg")

   # Example pattern for "works for" relationship
   def extract_employment_relations(text):
       doc = nlp(text)
       relations = []

       # Simple pattern: PERSON works/worked for ORGANIZATION
       for entity1 in doc.ents:
           if entity1.label_ == "PERSON":
               for entity2 in doc.ents:
                   if entity2.label_ == "ORG" and entity1 != entity2:
                       # Check if there's a work-related verb between them
                       for token in doc:
                           if (token.lemma_ in ["work", "employ"] and
                               token.i > entity1.end and
                               token.i < entity2.start):
                               relations.append({
                                   "subject": entity1.text,
                                   "relation": "works_for",
                                   "object": entity2.text
                               })
       return relations
   ```

2. **Supervised learning approach**:

   ```python
   # Simplified example of preparing data for supervised relation extraction
   def create_relation_features(sentence, e1_start, e1_end, e2_start, e2_end):
       # 1. Mark entities in sentence
       marked_text = (sentence[:e1_start] + "[E1]" + sentence[e1_start:e1_end] + "[/E1]" +
                     sentence[e1_end:e2_start] + "[E2]" + sentence[e2_start:e2_end] +
                     "[/E2]" + sentence[e2_end:])

       # 2. Get embeddings or other features
       # (In practice, you would use more sophisticated feature extraction)

       return features

   # In a real implementation, these features would be used to train a classifier
   ```

3. **Using pre-trained language models**:

   ```python
   from transformers import pipeline

   # Using a pre-trained relation extraction model
   relation_classifier = pipeline("text-classification",
                                 model="jean-baptiste/roberta-large-ner-english")

   def extract_relations_with_transformer(text, entities):
       relations = []

       # For each pair of entities
       for i, e1 in enumerate(entities):
           for j, e2 in enumerate(entities):
               if i != j:
                   # Construct input with special markers
                   marked_text = (text[:e1['start']] + "<e1>" + text[e1['start']:e1['end']] +
                                "</e1>" + text[e1['end']:e2['start']] + "<e2>" +
                                text[e2['start']:e2['end']] + "</e2>" + text[e2['end']:])

                   # Classify the relationship
                   result = relation_classifier(marked_text)

                   # Add if confidence is high enough
                   if result[0]['score'] > 0.7:
                       relations.append({
                           "subject": e1['text'],
                           "relation": result[0]['label'],
                           "object": e2['text'],
                           "confidence": result[0]['score']
                       })

       return relations
   ```

:::

Challenges in relationship extraction for knowledge graphs include:

1. **Relationship directionality**: Determining which entity is the subject and which is the object
2. **Implicit relationships**: Relationships that are implied but not explicitly stated
3. **Negated relationships**: Distinguishing between affirmed and negated relationships
4. **Relationship modality**: Capturing certainty, possibility, or necessity of relationships
5. **Distant entity pairs**: Extracting relationships between entities mentioned far apart in text

### Attribute extraction

Attribute extraction identifies properties and characteristics of entities:

::: {#def-attribute-extraction}

## Attribute extraction

**Attribute extraction** is the process of identifying properties or characteristics associated with entities. This involves:

1. Recognizing the entity and its attributes in text
2. Extracting the attribute values
3. Normalizing values to appropriate data types
4. Associating attributes with the correct entities

Common attribute types include:

- Quantitative attributes (age, price, measurements)
- Temporal attributes (date of birth, founding date)
- Descriptive attributes (color, material, status)
- Categorical attributes (nationality, genre, blood type)

:::

::: {#exm-attribute-extraction}

## Attribute extraction examples

1. **Product attributes**:

   ```
   Input: "The iPhone 13 Pro Max features a 6.7-inch display, weighs 240 grams, and comes with up to 1TB of storage."

   Extracted attributes:
   - Entity: iPhone 13 Pro Max (Type: Product)
   - display_size: 6.7 inches (Type: Measurement)
   - weight: 240 grams (Type: Measurement)
   - max_storage: 1TB (Type: Capacity)
   ```

2. **Person attributes**:

   ```
   Input: "Marie Curie (born Maria Salomea Skłodowska; 7 November 1867 – 4 July 1934) was a Polish-French physicist and chemist who conducted pioneering research on radioactivity."

   Extracted attributes:
   - Entity: Marie Curie (Type: Person)
   - birth_name: Maria Salomea Skłodowska (Type: Text)
   - birth_date: 7 November 1867 (Type: Date)
   - death_date: 4 July 1934 (Type: Date)
   - nationality: Polish-French (Type: Category)
   - profession: physicist, chemist (Type: Category)
   - research_field: radioactivity (Type: Category)
   ```

3. **Location attributes**:

   ```
   Input: "Tokyo, the capital of Japan, has a population of approximately 13.96 million and covers an area of 2,194 square kilometers."

   Extracted attributes:
   - Entity: Tokyo (Type: Location)
   - is_capital_of: Japan (Type: Relation)
   - population: 13.96 million (Type: Quantity)
   - area: 2,194 square kilometers (Type: Measurement)
   ```

:::

Attribute extraction approaches include:

1. **Pattern-based methods**:

   - Regular expressions for structured formats (e.g., dates, measurements)
   - Lexico-syntactic patterns (e.g., "X is Y years old")
   - Template filling based on attribute-value patterns
   - Advantages: Precision for well-structured attributes
   - Disadvantages: Limited coverage of expression variations

2. **Classification-based methods**:

   - Treating attribute extraction as sequence labeling
   - Using BIO (Beginning-Inside-Outside) tagging for attribute spans
   - Classifying attribute types for extracted values
   - Advantages: Better handling of context variation
   - Disadvantages: Requires labeled training data

3. **Information extraction frameworks**:
   - Joint extraction of entities and attributes
   - Leveraging dependency parsing for attribute-entity associations
   - Using ontologies to guide attribute extraction
   - Advantages: Coherent extraction of related information
   - Disadvantages: Complex implementation, error propagation

::: {#exm-attribute-implementation}

## Attribute extraction implementation

1. **Pattern-based attribute extraction**:

   ```python
   import re

   # Patterns for common attributes
   attribute_patterns = {
       'weight': r'weighs (\d+(?:\.\d+)?) (kg|grams|pounds|g)',
       'height': r'(stands|is) (\d+(?:\.\d+)?) (meters|m|cm|feet|ft) tall',
       'age': r'(\d+)(?:-| )years?(?:-| )old|aged (\d+)',
       'price': r'\$([\d,]+(?:\.\d+)?)|costs ([\d,]+(?:\.\d+)?) dollars'
   }

   def extract_attributes(text, entity):
       attributes = {}
       for attr_name, pattern in attribute_patterns.items():
           matches = re.finditer(pattern, text)
           for match in matches:
               # Extract the value (handling multiple capture groups)
               value = next(filter(None, match.groups()))
               unit = None
               # Extract unit if present in the pattern
               if attr_name in ['weight', 'height']:
                   unit = match.groups()[-1]

               attributes[attr_name] = {
                   'value': value,
                   'unit': unit,
                   'text': match.group(0)
               }
       return attributes
   ```

2. **Sequence labeling for attribute extraction**:

   ```python
   # Using spaCy for attribute extraction (simplified example)
   import spacy
   from spacy.tokens import DocBin
   import random

   # Example of preparing training data for a sequence labeling model
   def prepare_attribute_training_data():
       train_data = []

       # Example: "The book costs $15.99 and has 320 pages."
       # We'll label "costs $15.99" as PRICE and "320 pages" as PAGE_COUNT
       example1 = (
           "The book costs $15.99 and has 320 pages.",
           {"entities": [(10, 22, "PRICE"), (32, 41, "PAGE_COUNT")]}
       )
       train_data.append(example1)

       # More examples would be added here...

       return train_data

   # In practice, you would use this data to train a custom NER model
   # that recognizes attribute spans, then post-process to extract values
   ```

3. **Dependency parsing for attribute association**:

   ```python
   import spacy

   nlp = spacy.load("en_core_web_lg")

   def extract_attributes_with_dependencies(text, entity_span):
       doc = nlp(text)
       entity_tokens = []
       attributes = {}

       # Find entity tokens
       for token in doc:
           if token.idx >= entity_span[0] and token.idx + len(token.text) <= entity_span[1]:
               entity_tokens.append(token)

       if not entity_tokens:
           return attributes

       # Find attribute relationships using dependency parsing
       entity_head = entity_tokens[0]
       for token in doc:
           # Check for dependency relationships with entity
           if token.head in entity_tokens and token.dep_ in ["nummod", "amod"]:
               # Numerical or adjectival modifier
               attr_name = "numeric" if token.dep_ == "nummod" else "descriptive"
               attributes[attr_name] = token.text

           # Check for attribute phrases (e.g., "weighs 5 kg")
           if token.head in entity_tokens and token.dep_ == "acl":
               # Adjectival clause
               for child in token.children:
                   if child.dep_ == "dobj" and child.pos_ == "NUM":
                       attributes[token.lemma_] = child.text

       return attributes
   ```

:::

Challenges in attribute extraction for knowledge graphs include:

1. **Attribute-entity association**: Determining which attributes belong to which entities
2. **Value normalization**: Converting textual descriptions to standardized values and units
3. **Implicit attributes**: Inferring attributes that are assumed but not explicitly stated
4. **Attribute ambiguity**: Resolving cases where attributes could be interpreted in multiple ways
5. **Temporal aspects**: Capturing how attributes change over time

### Event extraction

Event extraction identifies and structures information about events and their participants:

::: {#def-event-extraction}

## Event extraction

**Event extraction** identifies events mentioned in text and extracts structured representations including:

1. **Event type**: The category or class of the event (e.g., acquisition, earthquake, election)
2. **Event trigger**: The word or phrase that indicates the event's occurrence
3. **Event arguments**: Entities playing specific roles in the event
4. **Temporal information**: When the event occurred
5. **Spatial information**: Where the event took place
6. **Additional attributes**: Other relevant details about the event

Events in knowledge graphs are typically represented as entities themselves, with relationships to participants and attributes.

:::

::: {#exm-event-extraction}

## Event extraction examples

1. **Business event**:

   ```
   Input: "Google acquired YouTube for $1.65 billion in October 2006."

   Extracted event:
   - Event_ID: Acquisition_123
   - Type: Acquisition
   - Trigger: "acquired"
   - Arguments:
     - Acquirer: Google
     - Acquired: YouTube
     - Amount: $1.65 billion
   - Time: October 2006
   ```

2. **Political event**:

   ```
   Input: "Joe Biden was inaugurated as the 46th President of the United States on January 20, 2021, in Washington, D.C."

   Extracted event:
   - Event_ID: Inauguration_456
   - Type: Inauguration
   - Trigger: "inaugurated"
   - Arguments:
     - Person: Joe Biden
     - Role: President
     - Organization: United States
   - Time: January 20, 2021
   - Location: Washington, D.C.
   ```

3. **Scientific event**:

   ```
   Input: "Researchers at Oxford University successfully tested a malaria vaccine with 77% efficacy in a phase 2b trial last year."

   Extracted event:
   - Event_ID: ClinicalTrial_789
   - Type: ClinicalTrial
   - Trigger: "tested"
   - Arguments:
     - Agent: Researchers
     - Affiliation: Oxford University
     - Subject: malaria vaccine
   - Attributes:
     - Efficacy: 77%
     - Phase: 2b
   - Time: last year
   ```

:::

Event extraction approaches include:

1. **Pipeline approaches**:

   - Sequential extraction of triggers, arguments, and attributes
   - Using classifiers for event type identification
   - Role labeling for argument identification
   - Advantages: Modular, easier to implement and debug
   - Disadvantages: Error propagation between stages

2. **Joint modeling approaches**:

   - Extracting triggers and arguments simultaneously
   - Using structured prediction models
   - Leveraging shared representations for related tasks
   - Advantages: Better handling of interdependencies
   - Disadvantages: More complex models, requires more training data

3. **Document-level approaches**:
   - Capturing events that span multiple sentences
   - Modeling event coreference and temporal ordering
   - Constructing narrative chains and scripts
   - Advantages: More comprehensive event representation
   - Disadvantages: Increased complexity, challenging evaluation

::: {#exm-event-implementation}

## Event extraction implementation

1. **Pipeline approach implementation**:

   ```python
   import spacy

   nlp = spacy.load("en_core_web_lg")

   def extract_events(text):
       doc = nlp(text)
       events = []

       # Step 1: Identify potential event triggers (primarily verbs)
       triggers = [token for token in doc if token.pos_ == "VERB"]

       for trigger in triggers:
           # Step 2: Classify event type (simplified)
           event_type = classify_event_type(trigger.lemma_)
           if not event_type:
               continue

           # Step 3: Extract arguments
           arguments = {}

           # Look for subject (typically agent)
           for child in trigger.children:
               if child.dep_ == "nsubj":
                   arguments["agent"] = extract_entity_span(child)

               # Look for object (typically patient/theme)
               elif child.dep_ in ["dobj", "attr"]:
                   arguments["theme"] = extract_entity_span(child)

               # Look for prepositional attachments
               elif child.dep_ == "prep":
                   prep_type = child.text
                   for prep_child in child.children:
                       if prep_child.dep_ == "pobj":
                           arg_name = f"{prep_type}_{prep_child.ent_type_}" if prep_child.ent_type_ else prep_type
                           arguments[arg_name] = extract_entity_span(prep_child)

           # Step 4: Extract temporal information
           time_exprs = [ent for ent in doc.ents if ent.label_ == "DATE" or ent.label_ == "TIME"]
           if time_exprs:
               arguments["time"] = time_exprs[0].text

           # Create the event
           event = {
               "type": event_type,
               "trigger": trigger.text,
               "trigger_pos": (trigger.idx, trigger.idx + len(trigger.text)),
               "arguments": arguments
           }
           events.append(event)

       return events

   def extract_entity_span(token):
       """Extract the full noun phrase for an entity"""
       if token.ent_type_:
           # If it's a named entity, use the entity span
           for ent in token.doc.ents:
               if token.i >= ent.start and token.i < ent.end:
                   return {"text": ent.text, "type": ent.label_}

       # Otherwise, try to get the full noun phrase
       if token.pos_ in ["NOUN", "PROPN"]:
           # Get the subtree span
           subtree = list(token.subtree)
           start = min(subtree, key=lambda x: x.i)
           end = max(subtree, key=lambda x: x.i)
           return {"text": doc[start.i:end.i+1].text, "type": "NP"}

       return {"text": token.text, "type": token.pos_}

   def classify_event_type(verb_lemma):
       """Simplified event type classification based on verb lemma"""
       event_types = {
           "acquire": "Acquisition",
           "buy": "Acquisition",
           "merge": "Merger",
           "announce": "Announcement",
           "launch": "ProductLaunch",
           "appoint": "Appointment",
           "resign": "Resignation",
           "elect": "Election",
           "inaugurate": "Inauguration",
           "discover": "Discovery",
           "test": "Test",
           "publish": "Publication"
       }
       return event_types.get(verb_lemma, None)
   ```

2. **Using a pre-trained event extraction model**:

   ```python
   # Using Hugging Face transformers for event extraction (conceptual example)
   from transformers import pipeline

   # This is conceptual - you would need to find or fine-tune an appropriate model
   event_extractor = pipeline("text-classification",
                             model="path/to/event/extraction/model")

   def extract_events_with_transformer(text):
       # Split text into appropriate chunks
       # (in practice, you would handle document chunking more carefully)
       chunks = [text[i:i+512] for i in range(0, len(text), 256)]

       all_events = []
       for chunk in chunks:
           # Identify event types and triggers
           events = event_extractor(chunk)

           for event in events:
               # Further processing to extract arguments
               # This would typically involve additional model calls or processing

               all_events.append({
                   "type": event["label"],
                   "confidence": event["score"],
                   "text": chunk[event["span"][0]:event["span"][1]],
                   # Arguments would be extracted here
               })

       return all_events
   ```

3. **Knowledge-driven event extraction**:

   ```python
   # Incorporating domain knowledge for event extraction

   # Sample event schema defining expected arguments for event types
   event_schemas = {
       "Acquisition": {
           "required_args": ["acquirer", "acquired"],
           "optional_args": ["amount", "time"],
           "trigger_verbs": ["acquire", "buy", "purchase", "take over"]
       },
       "Merger": {
           "required_args": ["org1", "org2"],
           "optional_args": ["time", "value"],
           "trigger_verbs": ["merge", "combine", "unite", "join"]
       }
       # More event types would be defined here
   }

   def knowledge_driven_event_extraction(doc, event_schemas):
       # Using the schemas to guide extraction
       # This would be combined with NLP techniques like those above
       # The schema helps ensure all required arguments are sought
       # and validates that extracted events are well-formed
       pass
   ```

:::

Challenges in event extraction for knowledge graphs include:

1. **Event complexity**: Events with multiple participants and nested structure
2. **Cross-sentence events**: Events described across multiple sentences
3. **Event coreference**: Identifying when different mentions refer to the same event
4. **Implicit events**: Events that are implied but not explicitly described
5. **Hypothetical or negated events**: Distinguishing between actual, potential, and non-events

### Domain-specific extraction techniques

Different domains require specialized extraction techniques to capture domain-specific entities and relationships:

::: {#def-domain-extraction}

## Domain-specific extraction

**Domain-specific extraction** involves tailored approaches for identifying entities, relationships, and attributes in specialized fields such as:

1. **Biomedical**: Genes, proteins, diseases, drugs, and their interactions
2. **Financial**: Companies, markets, financial instruments, transactions
3. **Legal**: Laws, cases, rulings, legal entities, obligations
4. **Scientific**: Experiments, measurements, theories, citations
5. **Technical**: Products, specifications, components, compatibilities

These approaches typically combine domain knowledge with specialized information extraction techniques.

:::

::: {#exm-domain-extraction}

## Domain-specific extraction examples

1. **Biomedical extraction**:

   ```
   Input: "PTEN mutations are associated with increased AKT phosphorylation, which contributes to tumorigenesis in prostate cancer."

   Extracted:
   - Entity: PTEN (Type: Gene)
   - Entity: AKT (Type: Protein)
   - Entity: prostate cancer (Type: Disease)
   - Relationship: associated_with(PTEN mutations, increased AKT phosphorylation)
   - Relationship: contributes_to(increased AKT phosphorylation, tumorigenesis)
   - Relationship: occurs_in(tumorigenesis, prostate cancer)
   ```

2. **Financial extraction**:

   ```
   Input: "Goldman Sachs downgraded Tesla stock from 'buy' to 'neutral' with a price target of $248, citing concerns about valuation."

   Extracted:
   - Entity: Goldman Sachs (Type: FinancialInstitution)
   - Entity: Tesla (Type: Company)
   - Entity: Tesla stock (Type: FinancialInstrument)
   - Event: StockDowngrade
     - Agent: Goldman Sachs
     - Target: Tesla stock
     - FromRating: buy
     - ToRating: neutral
     - PriceTarget: $248
     - Reason: "concerns about valuation"
   ```

3. **Legal extraction**:

   ```
   Input: "According to Section 230 of the Communications Decency Act, internet platforms are not liable for content posted by users."

   Extracted:
   - Entity: Section 230 (Type: LegalProvision)
   - Entity: Communications Decency Act (Type: Legislation)
   - Entity: internet platforms (Type: LegalEntity)
   - Relationship: part_of(Section 230, Communications Decency Act)
   - Rule: not_liable_for(internet platforms, content posted by users)
   - Authority: Section 230
   ```

:::

Domain-specific extraction approaches include:

1. **Specialized NER and relation extraction**:

   - Domain-specific entity types (e.g., genes, legal citations)
   - Custom relationship taxonomies
   - Tailored extraction patterns for domain conventions
   - Advantages: Precision for domain-specific concepts
   - Disadvantages: Development cost, limited transferability

2. **Dictionary and ontology-based methods**:

   - Using comprehensive terminological resources
   - Mapping to standardized identifiers
   - Leveraging domain ontologies for concept recognition
   - Advantages: Coverage of domain vocabulary, alignment with standards
   - Disadvantages: Maintenance of resources, novel term handling

3. **Transfer learning and domain adaptation**:
   - Fine-tuning general models on domain-specific data
   - Domain-adaptive pre-training on specialized corpora
   - Few-shot learning for rare entity types
   - Advantages: Leveraging general language understanding
   - Disadvantages: Requires domain-specific training data

::: {#exm-domain-implementation}

## Domain-specific extraction implementation

1. **Biomedical extraction using dictionaries and patterns**:

   ```python
   import re
   from scispacy.linking import EntityLinker
   import spacy

   # Load specialized biomedical model
   nlp = spacy.load("en_core_sci_lg")

   # Add biomedical entity linker
   linker = EntityLinker(resolve_abbreviations=True,
                         linker_name="umls")
   nlp.add_pipe(linker)

   # Additional patterns for relationships
   bio_relation_patterns = [
       {
           "name": "INHIBITS",
           "pattern": [
               {"LOWER": {"IN": ["inhibits", "suppresses", "blocks", "reduces"]}},
               {"OP": "*", "POS": {"NOT_IN": ["VERB"]}},
               {"ENT_TYPE": "CHEMICAL"}
           ]
       },
       # More patterns would be defined here
   ]

   def extract_biomedical_entities(text):
       doc = nlp(text)
       entities = []

       for ent in doc.ents:
           # Get linked concepts from UMLS
           linked_entities = [
               {
                   "id": f"UMLS:{e[0]}",
                   "name": nlp.vocab.strings[e[0]],
                   "score": e[1]
               }
               for e in ent._.kb_ents
           ]

           entities.append({
               "text": ent.text,
               "type": ent.label_,
               "pos": (ent.start_char, ent.end_char),
               "linked_entities": linked_entities[:3]  # Top 3 matches
           })

       return entities
   ```

2. **Financial extraction with domain-specific NER**:

   ```python
   # Using a fine-tuned financial NER model
   from transformers import AutoTokenizer, AutoModelForTokenClassification
   import torch

   # Load pre-trained financial NER model
   tokenizer = AutoTokenizer.from_pretrained("financial-bert-ner")
   model = AutoModelForTokenClassification.from_pretrained("financial-bert-ner")

   def extract_financial_entities(text):
       # Tokenize input
       inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)

       # Get predictions
       with torch.no_grad():
           outputs = model(**inputs)
           predictions = torch.argmax(outputs.logits, dim=2)

       # Convert token predictions to entities
       tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
       token_pred_labels = [model.config.id2label[t.item()] for t in predictions[0]]

       # Group tokens into entities (BIO format processing)
       entities = []
       current_entity = None

       for token, label in zip(tokens, token_pred_labels):
           if label.startswith("B-"):
               if current_entity:
                   entities.append(current_entity)
               entity_type = label[2:]  # Remove "B-" prefix
               current_entity = {"text": token, "type": entity_type}
           elif label.startswith("I-") and current_entity:
               # Continue current entity
               current_entity["text"] += " " + token
           elif label == "O":
               if current_entity:
                   entities.append(current_entity)
                   current_entity = None

       # Add last entity if there is one
       if current_entity:
           entities.append(current_entity)

       return entities
   ```

3. **Legal extraction with specialized patterns**:

   ```python
   # Example of legal citation extraction using regex patterns
   import re

   # Patterns for different types of legal citations
   citation_patterns = {
       "us_code": r"\d+\s+U\.?S\.?C\.?\s+§*\s*\d+",
       "cfr": r"\d+\s+C\.?F\.?R\.?\s+§*\s*\d+\.\d+",
       "scotus": r"(\d+)\s+U\.S\.\s+(\d+)\s+\((\d{4})\)",
       "public_law": r"Pub\.\s*L\.\s*No\.\s*\d+-\d+"
   }

   def extract_legal_citations(text):
       citations = []

       for citation_type, pattern in citation_patterns.items():
           for match in re.finditer(pattern, text):
               citations.append({
                   "text": match.group(0),
                   "type": citation_type,
                   "pos": (match.start(), match.end())
               })

       # Additional processing to normalize and categorize citations
       # would be added here

       return citations
   ```

:::

Challenges in domain-specific extraction include:

1. **Domain terminology complexity**: Highly specialized and technical vocabulary
2. **Disambiguation in context**: Terms with different meanings across domains
3. **Evolving terminology**: New terms and concepts emerging in rapidly developing fields
4. **Inter-domain connections**: Relationships spanning multiple specialized domains
5. **Domain resources**: Availability and quality of domain-specific knowledge resources

## Entity resolution and linking

Entity resolution and linking are crucial for creating coherent knowledge graphs by identifying when different mentions refer to the same real-world entity and connecting extracted entities to existing knowledge sources.

### Entity coreference resolution

Entity coreference resolution identifies when different mentions in text refer to the same entity:

::: {#def-coreference}

## Entity coreference resolution

**Entity coreference resolution** is the process of determining when multiple mentions in text refer to the same real-world entity. This involves:

1. Identifying all entity mentions in the text
2. Grouping mentions that refer to the same entity into coreference chains
3. Resolving pronouns and other referring expressions to their antecedents

Coreference resolution is essential for aggregating information about entities that may be mentioned multiple times throughout a document.

:::

::: {#exm-coreference}

## Coreference resolution examples

1. **Person coreference**:

   ```
   Text: "Tim Cook is the CEO of Apple. He joined the company in 1998 and became CEO after Steve Jobs."

   Coreference chains:
   - [Tim Cook, He] → Entity: Tim Cook
   - [Apple, the company] → Entity: Apple
   - [Steve Jobs] → Entity: Steve Jobs
   ```

2. **Organization coreference**:

   ```
   Text: "Microsoft announced new AI features. The tech giant has been investing heavily in artificial intelligence, and this move strengthens its position in the market."

   Coreference chains:
   - [Microsoft, The tech giant, its] → Entity: Microsoft
   - [new AI features, this move] → Entity: AI feature announcement
   ```

3. **Mixed entity types**:

   ```
   Text: "The researchers tested the new drug on cancer cells. They found that it reduced tumor growth by 60% in their experiments."

   Coreference chains:
   - [The researchers, They, their] → Entity: Researchers
   - [the new drug, it] → Entity: Drug
   - [cancer cells, tumor] → Entity: Cancer cells
   ```

:::

Coreference resolution approaches include:

1. **Rule-based approaches**:

   - Syntactic rules based on grammar
   - Semantic constraints based on entity types
   - Discourse-based heuristics
   - Advantages: Precision for common patterns
   - Disadvantages: Limited coverage, complex rule maintenance

2. **Machine learning approaches**:

   - Feature-based models using linguistic features
   - Neural network models with contextual representations
   - End-to-end models jointly identifying and resolving mentions
   - Advantages: Better handling of complex cases
   - Disadvantages: Require substantial training data

3. **Hybrid approaches**:
   - Combining rules and machine learning
   - Using rules for high-precision cases and ML for others
   - Advantages: Balance of precision and recall
   - Disadvantages: Increased system complexity

::: {#exm-coreference-implementation}

## Coreference resolution implementation

1. **Using spaCy for coreference resolution**:

   ```python
   # Using neuralcoref extension for spaCy
   import spacy
   import neuralcoref

   # Load model and add neural coreference resolution
   nlp = spacy.load("en_core_web_lg")
   neuralcoref.add_to_pipe(nlp)

   def resolve_coreferences(text):
       doc = nlp(text)

       # Extract coreference clusters
       clusters = []
       for cluster in doc._.coref_clusters:
           cluster_info = {
               "main": cluster.main.text,
               "mentions": [m.text for m in cluster.mentions],
               "positions": [(m.start_char, m.end_char) for m in cluster.mentions]
           }
           clusters.append(cluster_info)

       # Create resolved text
       resolved_text = doc._.coref_resolved

       return {
           "clusters": clusters,
           "resolved_text": resolved_text
       }
   ```

2. **Using Hugging Face transformers for coreference**:

   ```python
   from transformers import pipeline

   # Load coreference resolution pipeline
   coref = pipeline("coreference-resolution")

   def resolve_coreferences_with_transformers(text):
       # Process text through coreference pipeline
       result = coref(text)

       # Extract clusters from result
       clusters = []
       for cluster_id, mentions in result['clusters'].items():
           clusters.append({
               "id": cluster_id,
               "mentions": mentions
           })

       return {
           "clusters": clusters,
           "resolved_text": result['resolved_text']
       }
   ```

3. **Custom post-processing for knowledge graph integration**:
   ```python
   def integrate_coreferences_into_kg(extracted_entities, coreference_clusters):
       # Group entities by coreference cluster
       entity_clusters = {}

       for entity in extracted_entities:
           # Find if this entity belongs to any coreference cluster
           for cluster in coreference_clusters:
               entity_span = (entity["start"], entity["end"])

               if entity_span in cluster["positions"]:
                   cluster_id = cluster["main"]
                   if cluster_id not in entity_clusters:
                       entity_clusters[cluster_id] = []
                   entity_clusters[cluster_id].append(entity)
                   break

       # Merge entity information within each cluster
       merged_entities = []

       for cluster_id, entities in entity_clusters.items():
           if not entities:
               continue

           # Use the most informative mention as base
           base_entity = max(entities, key=lambda e: len(e.get("attributes", {})))

           # Merge attributes from all mentions
           merged_attributes = {}
           for entity in entities:
               merged_attributes.update(entity.get("attributes", {}))

           # Create merged entity
           merged_entity = {
               "id": base_entity["id"],
               "text": cluster_id,  # Use the main reference as the canonical text
               "type": base_entity["type"],
               "attributes": merged_attributes,
               "mentions": [e["text"] for e in entities]
           }

           merged_entities.append(merged_entity)

       # Add entities that weren't part of any cluster
       singleton_entities = [
           e for e in extracted_entities
           if not any(e in cluster for cluster in entity_clusters.values())
       ]

       return merged_entities + singleton_entities
   ```

:::

Challenges in coreference resolution for knowledge graphs include:

1. **Cross-document coreference**: Identifying references to the same entity across multiple documents
2. **Multiple entity types**: Handling different types of entities with appropriate resolution strategies
3. **Ambiguous references**: Resolving expressions that could refer to multiple potential antecedents
4. **Zero anaphora**: Handling cases where the referring expression is implicit or omitted
5. **Knowledge integration**: Combining linguistic coreference with knowledge-based entity resolution

### Entity linking to knowledge bases

Entity linking connects mentions in text to entities in existing knowledge bases:

::: {#def-entity-linking}

## Entity linking

**Entity linking** (also called named entity disambiguation) is the process of connecting entity mentions in text to their corresponding entries in a knowledge base or ontology. This involves:

1. Identifying entity mentions in text
2. Generating candidate entities from the knowledge base
3. Disambiguating between candidates based on context
4. Ranking candidates and selecting the most appropriate match

Entity linking provides standardized, unambiguous identifiers for entities, enabling integration with existing knowledge and cross-document entity resolution.

:::

::: {#exm-entity-linking}

## Entity linking examples

1. **Person linking**:

   ```
   Text mention: "Einstein developed the theory of relativity."

   Linked entity:
   - Knowledge base: DBpedia
   - Entity ID: http://dbpedia.org/resource/Albert_Einstein
   - Entity type: Scientist
   - Confidence: 0.98
   ```

2. **Location linking with ambiguity**:

   ```
   Text mention: "Paris is known for its fashion industry."

   Candidate entities:
   - http://dbpedia.org/resource/Paris (city in France) - Selected (0.95 confidence)
   - http://dbpedia.org/resource/Paris,_Texas (city in USA) - (0.03 confidence)
   - http://dbpedia.org/resource/Paris_Hilton (person) - (0.01 confidence)
   ```

3. **Organization linking with context**:

   ```
   Text mention: "Apple reported record profits this quarter."

   Context-based linking:
   - With financial context: http://dbpedia.org/resource/Apple_Inc. (0.97 confidence)
   - With agricultural context: http://dbpedia.org/resource/Apple (fruit) (0.92 confidence)
   ```

:::

Entity linking approaches include:

1. **Knowledge-based approaches**:

   - Using entity descriptions, types, and relationships
   - Computing semantic similarity between mention context and entity descriptions
   - Leveraging knowledge graph structure for disambiguation
   - Advantages: Interpretable, works with limited training data
   - Disadvantages: Depends on knowledge base coverage and quality

2. **Supervised learning approaches**:

   - Feature-based models using textual and knowledge-based features
   - Neural models using distributed representations
   - End-to-end models jointly performing NER and linking
   - Advantages: Better disambiguation performance
   - Disadvantages: Require substantial training data

3. **Collective approaches**:
   - Joint disambiguation of multiple entities in a document
   - Leveraging entity relationships for coherent linking
   - Global optimization across mentions
   - Advantages: More coherent entity linking across a document
   - Disadvantages: Computationally more intensive

::: {#exm-entity-linking-implementation}

## Entity linking implementation

1. **Simple entity linking with Wikipedia/DBpedia**:

   ```python
   import wikipedia
   import requests
   from nltk.tokenize import word_tokenize
   from nltk.corpus import stopwords
   from collections import Counter

   stop_words = set(stopwords.words('english'))

   def link_entity_to_wikipedia(entity_text, context_text):
       # Step 1: Generate candidate entities
       try:
           search_results = wikipedia.search(entity_text, results=5)
       except:
           return None

       if not search_results:
           return None

       candidates = []

       # Step 2: Get information about each candidate
       for title in search_results:
           try:
               # Get summary
               summary = wikipedia.summary(title, sentences=3)

               # Calculate context similarity
               # (simple bag-of-words approach)
               context_tokens = [w.lower() for w in word_tokenize(context_text)
                               if w.lower() not in stop_words]
               summary_tokens = [w.lower() for w in word_tokenize(summary)
                               if w.lower() not in stop_words]

               context_counter = Counter(context_tokens)
               summary_counter = Counter(summary_tokens)

               # Count overlapping terms
               overlap = sum((context_counter & summary_counter).values())

               # Calculate similarity score
               similarity = overlap / (len(summary_tokens) + 0.001)

               # Get DBpedia URI
               dbpedia_uri = f"http://dbpedia.org/resource/{title.replace(' ', '_')}"

               candidates.append({
                   "title": title,
                   "uri": dbpedia_uri,
                   "summary": summary,
                   "similarity": similarity
               })
           except:
               continue

       # Step 3: Rank candidates and select the best match
       if candidates:
           best_candidate = max(candidates, key=lambda x: x["similarity"])
           return best_candidate

       return None
   ```

2. **Using a pre-trained entity linking model**:

   ```python
   from transformers import pipeline

   # Load entity linking pipeline
   entity_linker = pipeline("entity_linking")

   def link_entities_with_transformers(text):
       # Process text through entity linking pipeline
       linked_entities = entity_linker(text)

       # Format results
       results = []
       for entity in linked_entities:
           results.append({
               "text": entity["word"],
               "start": entity["start"],
               "end": entity["end"],
               "uri": entity["uri"],
               "confidence": entity["score"]
           })

       return results
   ```

3. **Collective entity linking**:
   ```python
   def collective_entity_linking(entities, context_text, knowledge_graph):
       # Step 1: Generate candidate entities for each mention
       candidates_per_entity = {}

       for entity in entities:
           candidates = generate_candidates(entity["text"], knowledge_graph)
           candidates_per_entity[entity["id"]] = candidates

       # Step 2: Calculate local context similarity for each candidate
       for entity_id, candidates in candidates_per_entity.items():
           for candidate in candidates:
               candidate["local_score"] = calculate_context_similarity(
                   candidate["description"],
                   get_local_context(context_text, entities, entity_id)
               )

       # Step 3: Calculate coherence among candidate combinations
       # (simplified - real implementation would be more efficient)
       best_configuration = None
       best_score = -1

       # For small entity sets, brute force can work
       # For larger sets, optimization algorithms would be needed
       for configuration in generate_configurations(candidates_per_entity):
           coherence_score = calculate_coherence(configuration, knowledge_graph)
           local_score = sum(c["local_score"] for c in configuration.values())
           total_score = local_score + (coherence_score * coherence_weight)

           if total_score > best_score:
               best_score = total_score
               best_configuration = configuration

       # Link each entity to its selected candidate
       results = []
       for entity_id, candidate in best_configuration.items():
           entity = next(e for e in entities if e["id"] == entity_id)
           results.append({
               "entity": entity,
               "linked_entity": candidate
           })

       return results
   ```

:::

Challenges in entity linking for knowledge graphs include:

1. **Entity ambiguity**: Resolving mentions that could refer to multiple entities
2. **Knowledge base coverage**: Handling entities not present in the target knowledge base
3. **Limited context**: Disambiguation with minimal surrounding context
4. **Domain-specific entities**: Linking specialized entities in technical domains
5. **Emerging entities**: Handling new entities not yet in knowledge bases

### Cross-source entity resolution

Cross-source entity resolution identifies the same entities across different data sources:

::: {#def-entity-resolution}

## Cross-source entity resolution

**Cross-source entity resolution** (also called record linkage or entity matching) identifies when records from different data sources refer to the same real-world entity. This process involves:

1. Identifying potential matches using blocking or filtering techniques
2. Comparing entity attributes using similarity functions
3. Making match/non-match decisions based on similarity scores
4. Merging information from matched entities

Entity resolution is crucial when integrating data from multiple sources into a unified knowledge graph.

:::

::: {#exm-entity-resolution}

## Entity resolution examples

1. **Person resolution across databases**:

   ```
   Source 1: {id: "DB1_1234", name: "John Smith", email: "j.smith@example.com", title: "Senior Developer"}
   Source 2: {id: "DB2_5678", name: "J. Smith", email: "j.smith@example.com", role: "Lead Programmer"}

   Resolution: These records refer to the same person (match confidence: 0.95)
   Merged entity: {
     id: "KG_P9876",
     source_ids: ["DB1_1234", "DB2_5678"],
     name: "John Smith",
     email: "j.smith@example.com",
     title: "Senior Developer",
     role: "Lead Programmer"
   }
   ```

2. **Organization resolution with conflicts**:

   ```
   Source 1: {id: "SEC_123", name: "International Business Machines", ticker: "IBM", employees: 345000}
   Source 2: {id: "WIKI_456", name: "IBM Corporation", founded: 1911, employees: 350000}

   Resolution: These records refer to the same company (match confidence: 0.98)
   Merged entity with conflict resolution: {
     id: "KG_O5432",
     source_ids: ["SEC_123", "WIKI_456"],
     name: "International Business Machines",
     aliases: ["IBM Corporation"],
     ticker: "IBM",
     founded: 1911,
     employees: 350000,  // Taking the more recent or higher value
     employee_sources: {
       "SEC_123": 345000,
       "WIKI_456": 350000
     }
   }
   ```

3. **Product resolution with missing information**:

   ```
   Source 1: {id: "STORE_789", name: "iPhone 13", brand: "Apple", price: 799}
   Source 2: {id: "REVIEW_321", name: "Apple iPhone 13 (2021)", color: "Blue", storage: "128GB"}

   Resolution: These records refer to the same product (match confidence: 0.92)
   Merged entity: {
     id: "KG_I7890",
     source_ids: ["STORE_789", "REVIEW_321"],
     name: "iPhone 13",
     full_name: "Apple iPhone 13 (2021)",
     brand: "Apple",
     price: 799,
     color: "Blue",
     storage: "128GB",
     year: 2021
   }
   ```

:::

Entity resolution approaches include:

1. **Rule-based approaches**:

   - Deterministic matching based on exact field matches
   - Business rules for determining match/non-match
   - Manual resolution of borderline cases
   - Advantages: Transparent, high precision
   - Disadvantages: Labor-intensive, limited recall

2. **Similarity-based approaches**:

   - String similarity metrics (e.g., edit distance, Jaccard)
   - Phonetic similarity (e.g., Soundex, Metaphone)
   - Semantic similarity for textual fields
   - Advantages: Better handling of variations and errors
   - Disadvantages: Tuning similarity thresholds

3. **Machine learning approaches**:

   - Supervised classification of match/non-match
   - Active learning with human feedback
   - Representation learning for entity embeddings
   - Advantages: Can learn complex matching patterns
   - Disadvantages: Requires training data

4. **Graph-based approaches**:
   - Using relationship patterns for resolution
   - Propagating identity information through connections
   - Collective entity resolution across multiple entities
   - Advantages: Leverages structural information
   - Disadvantages: More complex, requires relationship data

::: {#exm-resolution-implementation}

## Entity resolution implementation

1. **Similarity-based entity resolution**:

   ```python
   import pandas as pd
   import numpy as np
   from recordlinkage.preprocessing import clean
   from recordlinkage.index import Block
   from recordlinkage.compare import Exact, String, Numeric
   import recordlinkage

   def resolve_entities(source1_df, source2_df):
       # Preprocessing
       source1_df['name_clean'] = clean(source1_df['name'])
       source2_df['name_clean'] = clean(source2_df['name'])

       # Indexing/blocking - find potential matches
       indexer = Block('name_clean', block_on='first_char')
       pairs = indexer.index(source1_df, source2_df)

       # Comparison - compute similarity features
       compare = recordlinkage.Compare()

       # Exact match on email
       compare.add(Exact('email', 'email', label='email'))

       # String similarity on name
       compare.add(String('name', 'name', label='name', method='jarowinkler'))

       # If other fields are present
       if 'phone' in source1_df.columns and 'phone' in source2_df.columns:
           compare.add(String('phone', 'phone', label='phone', method='levenshtein'))

       if 'address' in source1_df.columns and 'address' in source2_df.columns:
           compare.add(String('address', 'address', label='address', method='cosine'))

       # Compute similarity features
       features = compare.compute(pairs, source1_df, source2_df)

       # Classification - determine matches
       # Simple threshold-based classification
       matches = features[features.sum(axis=1) > 1.5]

       # Create merged entities
       merged_entities = []

       for index, row in matches.iterrows():
           source1_id, source2_id = index
           entity1 = source1_df.loc[source1_id].to_dict()
           entity2 = source2_df.loc[source2_id].to_dict()

           # Merge the entities
           merged = merge_entities(entity1, entity2)
           merged_entities.append(merged)

       return merged_entities

   def merge_entities(entity1, entity2):
       """Merge two entities, handling conflicts and combining information"""
       merged = {}

       # Generate a new ID
       merged['id'] = f"KG_{len(merged)}"
       merged['source_ids'] = [entity1.get('id'), entity2.get('id')]

       # Merge attributes with conflict resolution
       all_keys = set(entity1.keys()).union(set(entity2.keys()))

       for key in all_keys:
           # Skip metadata fields
           if key in ['id', 'source_ids', 'name_clean']:
               continue

           value1 = entity1.get(key)
           value2 = entity2.get(key)

           if value1 is None and value2 is not None:
               merged[key] = value2
           elif value1 is not None and value2 is None:
               merged[key] = value1
           elif value1 == value2:
               merged[key] = value1
           else:
               # Handle conflicts
               if key == 'name':
                   # Keep the longer name as primary, shorter as alias
                   if len(str(value1)) >= len(str(value2)):
                       merged[key] = value1
                       merged.setdefault('aliases', []).append(value2)
                   else:
                       merged[key] = value2
                       merged.setdefault('aliases', []).append(value1)
               elif key in ['phone', 'email']:
                   # For contact info, keep a list of all values
                   merged[key] = [value for value in [value1, value2] if value]
               elif isinstance(value1, (int, float)) and isinstance(value2, (int, float)):
                   # For numeric values, take the maximum (or could average)
                   merged[key] = max(value1, value2)
                   # Store original values for provenance
                   merged[f"{key}_sources"] = {
                       entity1.get('id', 'source1'): value1,
                       entity2.get('id', 'source2'): value2
                   }
               else:
                   # Default conflict resolution - keep both with source info
                   merged[f"{key}_source1"] = value1
                   merged[f"{key}_source2"] = value2

       return merged
   ```

2. **Machine learning-based entity resolution**:

   ```python
   import pandas as pd
   from sklearn.ensemble import RandomForestClassifier
   from recordlinkage.preprocessing import clean
   from recordlinkage.index import Block
   from recordlinkage.compare import Exact, String, Numeric
   import recordlinkage

   def ml_entity_resolution(source1_df, source2_df, training_matches):
       # Preprocessing
       source1_df['name_clean'] = clean(source1_df['name'])
       source2_df['name_clean'] = clean(source2_df['name'])

       # Indexing/blocking - find potential matches
       indexer = Block('name_clean', block_on='first_char')
       pairs = indexer.index(source1_df, source2_df)

       # Comparison - compute similarity features
       compare = recordlinkage.Compare()

       # Add comparison methods for each field
       compare.add(String('name', 'name', method='jarowinkler'))
       compare.add(String('name', 'name', method='levenshtein'))
       compare.add(Exact('email', 'email'))

       # Add more comparisons for other fields
       if 'phone' in source1_df.columns and 'phone' in source2_df.columns:
           compare.add(String('phone', 'phone', method='levenshtein'))

       # Compute similarity features
       features = compare.compute(pairs, source1_df, source2_df)

       # Create a training dataset from known matches
       train_indices = []
       for source1_id, source2_id in training_matches:
           if (source1_id, source2_id) in features.index:
               train_indices.append((source1_id, source2_id))

       # Create training data
       train_features = features.loc[train_indices]
       train_labels = [1] * len(train_indices)  # 1 for match

       # Add negative examples (non-matches)
       non_matches = [(idx1, idx2) for idx1, idx2 in features.index
                      if (idx1, idx2) not in train_indices]

       # Sample a subset of non-matches (to balance dataset)
       import random
       sampled_non_matches = random.sample(non_matches, min(len(train_indices), len(non_matches)))

       train_features = pd.concat([train_features, features.loc[sampled_non_matches]])
       train_labels.extend([0] * len(sampled_non_matches))

       # Train a classifier
       clf = RandomForestClassifier(n_estimators=100)
       clf.fit(train_features.values, train_labels)

       # Predict matches
       match_probabilities = clf.predict_proba(features.values)[:,1]
       matches_index = features.index[match_probabilities >= 0.7]  # Threshold

       # Create merged entities
       merged_entities = []

       for source1_id, source2_id in matches_index:
           entity1 = source1_df.loc[source1_id].to_dict()
           entity2 = source2_df.loc[source2_id].to_dict()
           confidence = match_probabilities[features.index.get_loc((source1_id, source2_id))]

           # Merge the entities
           merged = merge_entities(entity1, entity2)
           merged['match_confidence'] = confidence
           merged_entities.append(merged)

       return merged_entities
   ```

3. **Graph-based collective entity resolution**:
   ```python
   def collective_entity_resolution(entities_df, relationships_df):
       """
       Entity resolution that leverages relationship information

       entities_df: DataFrame with entity attributes
       relationships_df: DataFrame with relationships between entities
       """
       # Step 1: Initial similarity-based matching
       # (using techniques from previous examples)
       initial_matches = initial_similarity_matching(entities_df)

       # Step 2: Build entity-relationship graph
       G = build_relationship_graph(entities_df, relationships_df)

       # Step 3: Iterative collective resolution
       all_matches = set(initial_matches)
       previous_size = 0

       while len(all_matches) > previous_size:
           previous_size = len(all_matches)

           # For each potential match pair not yet decided
           for entity1_id, entity2_id in get_candidate_pairs(entities_df):
               if (entity1_id, entity2_id) in all_matches:
                   continue

               # Get relationship neighborhood for both entities
               neighbors1 = get_neighborhood(G, entity1_id)
               neighbors2 = get_neighborhood(G, entity2_id)

               # Count how many neighbors are matched to each other
               matched_neighbors = 0
               for n1 in neighbors1:
                   for n2 in neighbors2:
                       if (n1, n2) in all_matches or (n2, n1) in all_matches:
                           matched_neighbors += 1

               # Calculate attribute similarity
               attr_sim = calculate_similarity(
                   entities_df.loc[entity1_id],
                   entities_df.loc[entity2_id]
               )

               # Combined similarity score
               rel_factor = matched_neighbors / (len(neighbors1) + len(neighbors2) + 0.1)
               combined_score = (attr_sim + rel_factor) / 2

               # If score is high enough, consider it a match
               if combined_score >= 0.8:
                   all_matches.add((entity1_id, entity2_id))

       # Step 4: Create merged entities
       merged_entities = []
       for entity1_id, entity2_id in all_matches:
           entity1 = entities_df.loc[entity1_id].to_dict()
           entity2 = entities_df.loc[entity2_id].to_dict()
           merged = merge_entities(entity1, entity2)
           merged_entities.append(merged)

       return merged_entities
   ```

:::

Challenges in cross-source entity resolution include:

1. **Scalability**: Efficiently comparing large numbers of entities
2. **Missing data**: Resolving entities with incomplete information
3. **Conflicting information**: Handling inconsistencies across sources
4. **Evolving entities**: Entities that change over time across sources
5. **Privacy concerns**: Resolving entities while respecting privacy regulations

### Identity management and entity reconciliation

Identity management and entity reconciliation establish and maintain consistent entity identities across the knowledge graph:

::: {#def-identity-management}

## Identity management and reconciliation

**Identity management** in knowledge graphs involves creating and maintaining a consistent system for:

1. Assigning unique identifiers to entities
2. Managing entity lifecycles as information evolves
3. Tracking identity mappings across sources
4. Resolving conflicts in entity information

**Entity reconciliation** is the ongoing process of:

1. Merging information from different sources into coherent entity profiles
2. Maintaining provenance for entity attributes
3. Versioning entity information over time
4. Establishing confidence levels for entity information

:::

::: {#exm-identity-management}

## Identity management examples

1. **Entity identifier system**:

   ```
   Local ID:
   - Source 1: source1:person:12345 (internal database ID)
   - Source 2: source2:employee:E7890 (HR system ID)
   - Source 3: source3:author:smith_j (publication system ID)

   Global ID:
   - Knowledge Graph ID: kg:person:P58742
   - External IDs: orcid:0000-0002-1825-0097, wikidata:Q12345

   ID mappings:
   {
     "kg:person:P58742": {
       "local_ids": [
         "source1:person:12345",
         "source2:employee:E7890",
         "source3:author:smith_j"
       ],
       "external_ids": [
         "orcid:0000-0002-1825-0097",
         "wikidata:Q12345"
       ]
     }
   }
   ```

2. **Entity versioning and temporal tracking**:

   ```
   Entity: kg:company:C24680

   Version history:
   {
     "2022-01-15": {
       "name": "TechCorp Inc.",
       "headquarters": "Seattle, WA",
       "employees": 1250,
       "ceo": "kg:person:P12345"
     },
     "2022-06-30": {
       "name": "TechCorp Inc.",
       "headquarters": "Austin, TX",  // Changed headquarters
       "employees": 1350,
       "ceo": "kg:person:P12345"
     },
     "2023-02-10": {
       "name": "TechCorp Inc.",
       "headquarters": "Austin, TX",
       "employees": 1520,
       "ceo": "kg:person:P67890"  // New CEO
     }
   }
   ```

3. **Attribute-level provenance and confidence**:

   ```
   Entity: kg:medication:M36912

   Attributes with provenance:
   {
     "name": {
       "value": "Acetylsalicylic Acid",
       "sources": [
         {"id": "source1:drug:D123", "confidence": 1.0},
         {"id": "source2:med:M456", "confidence": 1.0}
       ]
     },
     "common_name": {
       "value": "Aspirin",
       "sources": [
         {"id": "source1:drug:D123", "confidence": 1.0},
         {"id": "source3:wiki:aspirin", "confidence": 1.0}
       ]
     },
     "side_effects": {
       "value": ["Stomach irritation", "Nausea", "Ringing in ears"],
       "sources": [
         {"id": "source1:drug:D123", "confidence": 0.9},
         {"id": "source4:clinical:C789", "confidence": 0.95}
       ]
     },
     "recommended_dosage": {
       "value": "75-325 mg daily",
       "sources": [
         {"id": "source1:drug:D123", "confidence": 0.85},
         {"id": "source4:clinical:C789", "confidence": 0.9}
       ],
       "conflicts": [
         {"source": "source5:alternative:A101", "value": "50-100 mg daily"}
       ]
     }
   }
   ```

:::

Key aspects of implementing identity management include:

1. **Global identifier system**:

   - Persistent, unique IDs independent of source systems
   - URI-based identifiers for web compatibility
   - Namespacing to organize entity types
   - Mappings to external identifier systems

2. **Provenance tracking**:

   - Source attribution for all information
   - Confidence scoring for attributes
   - Timestamping for temporal tracking
   - Conflict documentation for disagreements

3. **Entity merging policies**:

   - Rules for combining information from different sources
   - Strategies for handling conflicting information
   - Thresholds for establishing entity identity
   - Versioning policies for temporal changes

4. **Governance processes**:
   - Workflows for manual resolution of ambiguous cases
   - Quality control procedures for entity information
   - Access control for identity management functions
   - Audit trails for identity decisions

::: {#exm-identity-implementation}

## Identity management implementation

1. **Global identifier service**:

   ```python
   import uuid
   import datetime
   import json

   class EntityIdentityService:
       def __init__(self, db_connection):
           self.db = db_connection
           self.namespace_prefix = "kg:"

       def generate_entity_id(self, entity_type):
           """Generate a new global entity ID"""
           # Create a type-specific prefix
           type_prefix = f"{self.namespace_prefix}{entity_type}:"

           # Generate a unique identifier
           unique_part = str(uuid.uuid4()).replace('-', '')[:8].upper()

           # Combine to create the full ID
           entity_id = f"{type_prefix}{unique_part}"

           return entity_id

       def register_entity(self, entity_type, source_id, attributes):
           """Register a new entity or update an existing one"""
           # Check if this source_id is already mapped to a global ID
           existing_mapping = self.lookup_by_source_id(source_id)

           if existing_mapping:
               # Entity already exists, update attributes
               global_id = existing_mapping["global_id"]
               self.update_entity_attributes(global_id, source_id, attributes)
               return global_id
           else:
               # Create new entity
               global_id = self.generate_entity_id(entity_type)

               # Create initial mapping
               mapping = {
                   "global_id": global_id,
                   "entity_type": entity_type,
                   "local_ids": [source_id],
                   "external_ids": [],
                   "created_at": datetime.datetime.utcnow().isoformat(),
                   "last_updated": datetime.datetime.utcnow().isoformat()
               }

               # Store the mapping
               self.db.store_identity_mapping(global_id, mapping)

               # Store initial attributes
               self.update_entity_attributes(global_id, source_id, attributes)

               return global_id

       def merge_entities(self, global_id1, global_id2):
           """Merge two entities that are determined to be the same"""
           # Get the current mappings
           mapping1 = self.lookup_by_global_id(global_id1)
           mapping2 = self.lookup_by_global_id(global_id2)

           if not mapping1 or not mapping2:
               raise ValueError("One or both entities not found")

           # Create merged mapping
           merged_mapping = {
               "global_id": global_id1,  # Keep the first ID as the canonical ID
               "entity_type": mapping1["entity_type"],
               "local_ids": list(set(mapping1["local_ids"] + mapping2["local_ids"])),
               "external_ids": list(set(mapping1["external_ids"] + mapping2["external_ids"])),
               "created_at": min(mapping1["created_at"], mapping2["created_at"]),
               "last_updated": datetime.datetime.utcnow().isoformat(),
               "merged_from": [global_id1, global_id2]
           }

           # Store the merged mapping
           self.db.store_identity_mapping(global_id1, merged_mapping)

           # Create redirect for the second ID
           redirect_mapping = {
               "global_id": global_id2,
               "redirect_to": global_id1,
               "last_updated": datetime.datetime.utcnow().isoformat()
           }
           self.db.store_identity_mapping(global_id2, redirect_mapping)

           # Merge attributes
           self.merge_entity_attributes(global_id1, global_id2)

           return global_id1

       def lookup_by_source_id(self, source_id):
           """Find global ID by source ID"""
           return self.db.find_mapping_by_source_id(source_id)

       def lookup_by_global_id(self, global_id):
           """Get mapping by global ID"""
           mapping = self.db.get_identity_mapping(global_id)

           # Follow redirects if present
           if mapping and "redirect_to" in mapping:
               return self.lookup_by_global_id(mapping["redirect_to"])

           return mapping

       def update_entity_attributes(self, global_id, source_id, attributes):
           """Update entity attributes with provenance"""
           # Get current attributes
           current_attributes = self.db.get_entity_attributes(global_id) or {}

           # Update each attribute with provenance
           timestamp = datetime.datetime.utcnow().isoformat()

           for key, value in attributes.items():
               if key not in current_attributes:
                   # New attribute
                   current_attributes[key] = {
                       "value": value,
                       "sources": [
                           {"id": source_id, "timestamp": timestamp, "confidence": 1.0}
                       ]
                   }
               else:
                   # Update existing attribute
                   current_value = current_attributes[key]["value"]
                   current_sources = current_attributes[key]["sources"]

                   # Check if this source already provided this attribute
                   source_exists = False
                   for source in current_sources:
                       if source["id"] == source_id:
                           source["timestamp"] = timestamp
                           source_exists = True
                           break

                   if not source_exists:
                       current_sources.append({
                           "id": source_id,
                           "timestamp": timestamp,
                           "confidence": 1.0
                       })

                   # Handle conflicts
                   if value != current_value:
                       # Different value - use conflict resolution policy
                       resolved_value, conflicts = self.resolve_attribute_conflict(
                           key, current_value, value, current_sources
                       )

                       current_attributes[key]["value"] = resolved_value
                       if conflicts:
                           current_attributes[key]["conflicts"] = conflicts

           # Store updated attributes
           self.db.store_entity_attributes(global_id, current_attributes)

       def resolve_attribute_conflict(self, attribute_key, current_value, new_value, sources):
           """Implement conflict resolution policies"""
           # Different policies depending on attribute type
           if attribute_key in ["name", "title", "id"]:
               # For identifiers, prefer the value with most sources
               current_sources = [s for s in sources if s.get("value", current_value) == current_value]
               new_sources = [s for s in sources if s.get("value", None) == new_value]

               if len(current_sources) >= len(new_sources):
                   return current_value, [{"value": new_value, "sources": new_sources}]
               else:
                   return new_value, [{"value": current_value, "sources": current_sources}]

           elif attribute_key in ["population", "revenue", "count"]:
               # For numeric values, prefer the most recent
               most_recent_source = max(sources, key=lambda s: s["timestamp"])
               most_recent_value = most_recent_source.get("value", current_value)

               if most_recent_value == current_value:
                   return current_value, [{"value": new_value, "reason": "older"}]
               else:
                   return new_value, [{"value": current_value, "reason": "older"}]

           else:
               # Default policy: keep lists of values
               if isinstance(current_value, list):
                   if isinstance(new_value, list):
                       combined = list(set(current_value + new_value))
                       return combined, []
                   else:
                       if new_value not in current_value:
                           current_value.append(new_value)
                       return current_value, []
               else:
                   # Convert to list if values differ
                   if current_value != new_value:
                       return [current_value, new_value], []
                   else:
                       return current_value, []
   ```

2. **Entity versioning and temporal tracking**:

   ```python
   class EntityVersionService:
       def __init__(self, db_connection):
           self.db = db_connection

       def create_version(self, entity_id, attributes, timestamp=None):
           """Create a new version of an entity"""
           # Use current time if no timestamp provided
           if timestamp is None:
               timestamp = datetime.datetime.utcnow().isoformat()

           # Get existing versions
           versions = self.get_all_versions(entity_id)

           # Create new version
           new_version = {
               "entity_id": entity_id,
               "timestamp": timestamp,
               "attributes": attributes,
               "created_at": datetime.datetime.utcnow().isoformat()
           }

           # Store the new version
           self.db.store_entity_version(entity_id, timestamp, new_version)

           return new_version

       def get_all_versions(self, entity_id):
           """Get all versions of an entity, ordered by timestamp"""
           versions = self.db.get_entity_versions(entity_id)
           return sorted(versions, key=lambda v: v["timestamp"])

       def get_version_at(self, entity_id, timestamp):
           """Get entity state at specific timestamp"""
           versions = self.get_all_versions(entity_id)

           # Find the version active at the given timestamp
           active_version = None
           for version in versions:
               if version["timestamp"] <= timestamp:
                   active_version = version
               else:
                   break

           return active_version

       def diff_versions(self, entity_id, timestamp1, timestamp2):
           """Compare entity state between two timestamps"""
           v1 = self.get_version_at(entity_id, timestamp1)
           v2 = self.get_version_at(entity_id, timestamp2)

           if not v1 or not v2:
               return None

           # Compare attributes
           changes = {}

           # Find modified attributes
           all_keys = set(v1["attributes"].keys()).union(set(v2["attributes"].keys()))

           for key in all_keys:
               v1_value = v1["attributes"].get(key)
               v2_value = v2["attributes"].get(key)

               if v1_value != v2_value:
                   changes[key] = {
                       "from": v1_value,
                       "to": v2_value
                   }

           return changes
   ```

3. **Entity reconciliation service**:
   ```python
   class EntityReconciliationService:
       def __init__(self, identity_service, db_connection):
           self.identity_service = identity_service
           self.db = db_connection

       def reconcile_entity(self, entity_data, source_id, confidence_threshold=0.8):
           """
           Reconcile an entity from a source against the knowledge graph
           """
           # Step 1: Extract key attributes for matching
           match_attributes = self.extract_match_attributes(entity_data)

           # Step 2: Search for potential matches
           candidate_matches = self.find_candidate_matches(match_attributes)

           # Step 3: Calculate similarity scores
           scored_matches = []
           for candidate in candidate_matches:
               similarity = self.calculate_similarity(match_attributes, candidate["attributes"])
               scored_matches.append({
                   "global_id": candidate["global_id"],
                   "similarity": similarity,
                   "attributes": candidate["attributes"]
               })

           # Step 4: Select best match or create new entity
           scored_matches.sort(key=lambda m: m["similarity"], reverse=True)

           if scored_matches and scored_matches[0]["similarity"] >= confidence_threshold:
               # Match found - update existing entity
               best_match = scored_matches[0]
               global_id = best_match["global_id"]

               # Update the entity with new data
               self.identity_service.update_entity_attributes(
                   global_id, source_id, entity_data
               )

               return {
                   "global_id": global_id,
                   "action": "updated",
                   "similarity": best_match["similarity"]
               }
           else:
               # No good match - create new entity
               entity_type = self.determine_entity_type(entity_data)
               global_id = self.identity_service.register_entity(
                   entity_type, source_id, entity_data
               )

               return {
                   "global_id": global_id,
                   "action": "created",
                   "similarity": 0.0
               }

       def extract_match_attributes(self, entity_data):
           """Extract key attributes used for matching"""
           match_attributes = {}

           # Prioritize identifying attributes
           if "id" in entity_data:
               match_attributes["id"] = entity_data["id"]

           if "name" in entity_data:
               match_attributes["name"] = entity_data["name"]

           # Include other potential matching attributes
           for key in ["email", "date_of_birth", "phone", "address"]:
               if key in entity_data:
                   match_attributes[key] = entity_data[key]

           return match_attributes

       def find_candidate_matches(self, match_attributes):
           """Find potential matching entities based on key attributes"""
           candidates = []

           # Try exact matches on strong identifiers
           for key in ["id", "email"]:
               if key in match_attributes:
                   exact_matches = self.db.find_entities_by_attribute(
                       key, match_attributes[key]
                   )
                   candidates.extend(exact_matches)

           # Try fuzzy matches on names
           if "name" in match_attributes and len(candidates) < 5:
               name_matches = self.db.find_entities_by_name_similarity(
                   match_attributes["name"]
               )

               # Add only new candidates
               existing_ids = {c["global_id"] for c in candidates}
               candidates.extend([
                   c for c in name_matches if c["global_id"] not in existing_ids
               ])

           return candidates

       def calculate_similarity(self, attributes1, attributes2):
           """Calculate similarity between two sets of entity attributes"""
           # Implement similarity calculation logic
           # This could use string similarity for names, exact matching for emails,
           # specialized comparisons for addresses, etc.

           # Sample implementation (simplified)
           total_weight = 0
           weighted_sum = 0

           # Define weights for different attributes
           weights = {
               "id": 1.0,
               "email": 0.9,
               "name": 0.8,
               "date_of_birth": 0.7,
               "phone": 0.6,
               "address": 0.5
           }

           for key, weight in weights.items():
               if key in attributes1 and key in attributes2:
                   value1 = attributes1[key]
                   value2 = attributes2[key]

                   if key in ["id", "email"]:
                       # Exact match for identifiers
                       sim = 1.0 if value1 == value2 else 0.0
                   elif key == "name":
                       # String similarity for names
                       sim = self.string_similarity(value1, value2)
                   elif key == "date_of_birth":
                       # Exact match for dates
                       sim = 1.0 if value1 == value2 else 0.0
                   else:
                       # Default string similarity
                       sim = self.string_similarity(str(value1), str(value2))

                   weighted_sum += sim * weight
                   total_weight += weight

           if total_weight == 0:
               return 0.0

           return weighted_sum / total_weight

       def string_similarity(self, s1, s2):
           """Calculate string similarity (e.g., Jaro-Winkler)"""
           # Implement string similarity algorithm
           # This is a placeholder - would use an actual implementation
           from textdistance import jaro_winkler
           return jaro_winkler.normalized_similarity(s1, s2)

       def determine_entity_type(self, entity_data):
           """Determine the entity type based on available attributes"""
           # Implement logic to infer entity type
           # This could be based on the presence of certain attributes,
           # or an explicit type field in the data

           if "company_name" in entity_data or "industry" in entity_data:
               return "organization"
           elif "first_name" in entity_data or "date_of_birth" in entity_data:
               return "person"
           elif "isbn" in entity_data or "author" in entity_data:
               return "book"
           else:
               return "generic"
   ```

:::

Effective identity management in knowledge graphs provides several benefits:

1. **Consistency**: Unified view of entities across the knowledge graph
2. **Traceability**: Ability to track the origin of all entity information
3. **Flexibility**: Support for evolving entity information over time
4. **Confidence**: Clear indication of certainty in entity attributes
5. **Integration**: Ability to connect with external identifier systems

## Knowledge graph integration and fusion

Once entities and relationships have been extracted from various sources, they must be integrated into a coherent knowledge graph. This section covers techniques for combining and reconciling information from multiple sources.

### Schema alignment and mapping

Schema alignment creates correspondences between different data models:

::: {#def-schema-alignment}

## Schema alignment and mapping

**Schema alignment** (or schema matching) identifies correspondences between elements in different schemas or ontologies. This includes:

1. **Class alignment**: Mapping between entity types (e.g., Person ↔ Individual)
2. **Property alignment**: Mapping between relationships and attributes (e.g., authorOf ↔ wrote)
3. **Value mapping**: Transformations between different value representations (e.g., date formats)

**Schema mapping** uses these alignments to transform data from one schema to another, enabling integration of diverse data sources into a unified knowledge graph.

:::

::: {#exm-schema-alignment}

## Schema alignment examples

1. **Class mapping**:

   ```
   Source schema 1:
   - Class: Customer
     Properties: id, name, address, phone, email

   Source schema 2:
   - Class: Client
     Properties: client_id, full_name, contact_address, telephone, email_address

   Alignment:
   Customer ↔ Client
   id ↔ client_id
   name ↔ full_name
   address ↔ contact_address
   phone ↔ telephone
   email ↔ email_address
   ```

2. **Property mapping with transformations**:

   ```
   Source schema 1:
   - Property: birth_date (format: MM/DD/YYYY)

   Source schema 2:
   - Property: dateOfBirth (format: YYYY-MM-DD)

   Target schema:
   - Property: birthDate (format: YYYY-MM-DD)

   Mapping:
   birth_date → birthDate (with format transformation MM/DD/YYYY → YYYY-MM-DD)
   dateOfBirth → birthDate (direct mapping, formats already match)
   ```

3. **Complex mapping with composition**:

   ```
   Source schema:
   - Class: Address
     Properties: street, city, state, zip

   Target schema:
   - Class: Location
     Properties: full_address, coordinates

   Mapping:
   Address → Location
   concat(street, ", ", city, ", ", state, " ", zip) → full_address
   geocode(full_address) → coordinates
   ```

:::

Schema alignment approaches include:

1. **Manual mapping**:

   - Expert-created correspondences between schemas
   - Custom transformation rules for complex mappings
   - Advantages: Precision, domain knowledge incorporation
   - Disadvantages: Labor-intensive, difficult to scale

2. **Schema matching algorithms**:

   - Lexical matching based on element names and descriptions
   - Structural matching based on schema hierarchies and relationships
   - Instance-based matching using sample data values
   - Advantages: Automated, can handle large schemas
   - Disadvantages: Lower precision, may miss complex correspondences

3. **Mapping languages and tools**:
   - Languages like R2RML for mapping relational data to RDF
   - SPARQL-based mapping approaches
   - ETL tools with semantic mapping capabilities
   - Advantages: Standardized, reusable mappings
   - Disadvantages: Learning curve, implementation complexity

::: {#exm-schema-mapping-implementation}

## Schema mapping implementation

1. **R2RML mapping for relational to RDF transformation**:

   ```turtle
   @prefix rr: <http://www.w3.org/ns/r2rml#> .
   @prefix ex: <http://example.org/> .
   @prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

   # Define a mapping for the Customers table
   <#CustomerMapping>
     rr:logicalTable [ rr:tableName "Customers" ];

     # Define how to create URIs for each customer
     rr:subjectMap [
       rr:template "http://example.org/customer/{id}";
       rr:class ex:Customer;
     ];

     # Map simple properties
     rr:predicateObjectMap [
       rr:predicate ex:name;
       rr:objectMap [ rr:column "name" ];
     ];

     rr:predicateObjectMap [
       rr:predicate ex:email;
       rr:objectMap [ rr:column "email" ];
     ];

     # Map with datatype transformation
     rr:predicateObjectMap [
       rr:predicate ex:registrationDate;
       rr:objectMap [
         rr:column "reg_date";
         rr:datatype xsd:date;
       ];
     ];

     # Map with a reference to another entity
     rr:predicateObjectMap [
       rr:predicate ex:livesIn;
       rr:objectMap [
         rr:parentTriplesMap <#CityMapping>;
         rr:joinCondition [
           rr:child "city_id";
           rr:parent "id";
         ];
       ];
     ];
   .

   # Define a mapping for the Cities table
   <#CityMapping>
     rr:logicalTable [ rr:tableName "Cities" ];

     rr:subjectMap [
       rr:template "http://example.org/city/{id}";
       rr:class ex:City;
     ];

     rr:predicateObjectMap [
       rr:predicate ex:name;
       rr:objectMap [ rr:column "name" ];
     ];

     rr:predicateObjectMap [
       rr:predicate ex:country;
       rr:objectMap [ rr:column "country" ];
     ];
   .
   ```

2. **JSON schema to RDF mapping implementation**:

   ```python
   import json
   import rdflib
   from rdflib import Graph, Literal, URIRef, Namespace
   from rdflib.namespace import RDF, RDFS, XSD

   def json_to_rdf(json_data, mapping_config, base_uri):
       """
       Transform JSON data to RDF based on a mapping configuration

       json_data: JSON data to transform
       mapping_config: Configuration defining how to map JSON to RDF
       base_uri: Base URI for creating entity identifiers
       """
       # Create RDF graph
       g = Graph()

       # Define namespaces
       EX = Namespace(base_uri)
       g.bind("ex", EX)

       # Process all records in the JSON data
       if isinstance(json_data, list):
           # Array of objects
           for item in json_data:
               process_json_object(g, item, mapping_config, EX)
       else:
           # Single object
           process_json_object(g, json_data, mapping_config, EX)

       return g

   def process_json_object(graph, obj, mapping, ns):
       """Process a single JSON object according to the mapping"""
       # Determine the entity type
       entity_type = mapping.get("type", "Entity")

       # Generate entity URI
       id_field = mapping.get("id_field", "id")
       entity_id = obj.get(id_field, str(hash(json.dumps(obj, sort_keys=True))))
       entity_uri = ns[f"{entity_type}/{entity_id}"]

       # Add entity type
       graph.add((entity_uri, RDF.type, ns[entity_type]))

       # Process properties
       properties = mapping.get("properties", {})
       for prop_name, prop_config in properties.items():
           if prop_name in obj:
               # Get the value from the JSON object
               value = obj[prop_name]

               # Get property URI and datatype
               prop_uri = ns[prop_config.get("uri", prop_name)]
               datatype = prop_config.get("datatype", None)

               if prop_config.get("type") == "object":
                   # Handle nested object
                   if value:
                       nested_mapping = prop_config.get("mapping", {"type": prop_name})
                       process_json_object(graph, value, nested_mapping, ns)

                       # Create relationship to nested entity
                       nested_id_field = nested_mapping.get("id_field", "id")
                       nested_type = nested_mapping.get("type", prop_name)
                       nested_id = value.get(nested_id_field, str(hash(json.dumps(value, sort_keys=True))))
                       nested_uri = ns[f"{nested_type}/{nested_id}"]

                       graph.add((entity_uri, prop_uri, nested_uri))

               elif prop_config.get("type") == "array":
                   # Handle array values
                   item_mapping = prop_config.get("items", {})

                   if isinstance(value, list):
                       for item in value:
                           if item_mapping.get("type") == "object":
                               # Array of objects - create linked entities
                               process_json_object(graph, item, item_mapping, ns)

                               # Create relationship to each item
                               item_type = item_mapping.get("type", "Item")
                               item_id_field = item_mapping.get("id_field", "id")
                               item_id = item.get(item_id_field, str(hash(json.dumps(item, sort_keys=True))))
                               item_uri = ns[f"{item_type}/{item_id}"]

                               graph.add((entity_uri, prop_uri, item_uri))
                           else:
                               # Array of literals - add each value
                               literal = create_literal(item, item_mapping.get("datatype"))
                               graph.add((entity_uri, prop_uri, literal))
               else:
                   # Handle simple literal properties
                   literal = create_literal(value, datatype)
                   graph.add((entity_uri, prop_uri, literal))

       return entity_uri

   def create_literal(value, datatype):
       """Create an RDF literal with appropriate datatype"""
       if datatype == "xsd:string" or datatype is None:
           return Literal(value, datatype=XSD.string)
       elif datatype == "xsd:integer":
           return Literal(int(value), datatype=XSD.integer)
       elif datatype == "xsd:float":
           return Literal(float(value), datatype=XSD.float)
       elif datatype == "xsd:boolean":
           return Literal(bool(value), datatype=XSD.boolean)
       elif datatype == "xsd:date":
           return Literal(value, datatype=XSD.date)
       elif datatype == "xsd:dateTime":
           return Literal(value, datatype=XSD.dateTime)
       else:
           return Literal(value)
   ```

3. **SPARQL-based mapping**:

   ```sparql
   # SPARQL CONSTRUCT query to transform data from one schema to another
   PREFIX src: <http://source-schema.org/>
   PREFIX tgt: <http://target-schema.org/>
   PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

   CONSTRUCT {
     # Create entities in target schema
     ?personURI a tgt:Person ;
          tgt:fullName ?name ;
          tgt:emailAddress ?email ;
          tgt:birthDate ?formattedDate ;
          tgt:livesAt ?addressURI .

     ?addressURI a tgt:Location ;
          tgt:streetAddress ?street ;
          tgt:locality ?city ;
          tgt:region ?state ;
          tgt:postalCode ?zip ;
          tgt:country "USA" .
   }
   WHERE {
     # Source data pattern
     ?sourceURI a src:Customer ;
          src:givenName ?firstName ;
          src:familyName ?lastName ;
          src:email ?email ;
          src:dateOfBirth ?dob .

     OPTIONAL {
       ?sourceURI src:hasAddress ?srcAddr .
       ?srcAddr src:street ?street ;
               src:city ?city ;
               src:state ?state ;
               src:zipCode ?zip .
     }

     # Transformations
     BIND(CONCAT(?firstName, " ", ?lastName) AS ?name)
     BIND(URI(CONCAT("http://target-schema.org/person/", ENCODE_FOR_URI(?name))) AS ?personURI)
     BIND(URI(CONCAT("http://target-schema.org/location/", MD5(CONCAT(?street, ?city, ?state, ?zip)))) AS ?addressURI)

     # Format date from MM/DD/YYYY to YYYY-MM-DD
     BIND(IF(CONTAINS(?dob, "/"),
             CONCAT(SUBSTR(?dob, 7, 4), "-", SUBSTR(?dob, 0, 2), "-", SUBSTR(?dob, 4, 2)),
             ?dob) AS ?formattedDate)
   }
   ```

:::

Challenges in schema alignment and mapping include:

1. **Semantic heterogeneity**: Different conceptual models of the same domain
2. **Structural differences**: Variations in how concepts are organized
3. **Complex transformations**: Need for custom logic to handle certain mappings
4. **Maintenance**: Keeping mappings up to date as schemas evolve
5. **Scalability**: Managing alignments across many sources and large schemas

### Knowledge fusion strategies

Knowledge fusion combines information from multiple sources while resolving conflicts and ensuring coherence:

::: {#def-knowledge-fusion}

## Knowledge fusion

**Knowledge fusion** is the process of combining information from multiple sources into a unified, coherent knowledge representation. Key aspects include:

1. **Information aggregation**: Collecting and combining complementary information
2. **Conflict resolution**: Reconciling contradictory information
3. **Source reliability assessment**: Weighting information based on source credibility
4. **Temporal resolution**: Handling information from different time periods
5. **Uncertainty representation**: Capturing confidence in fused knowledge

:::

::: {#exm-knowledge-fusion}

## Knowledge fusion examples

1. **Complementary information fusion**:

   ```
   Source 1: {id: "person/123", name: "John Smith", birthDate: "1980-05-15"}
   Source 2: {id: "person/456", name: "John Smith", profession: "Software Engineer"}
   Source 3: {id: "person/789", name: "J. Smith", email: "john.smith@example.com"}

   After entity resolution and fusion:
   {
     id: "kg:person:P12345",
     name: "John Smith",
     birthDate: "1980-05-15",
     profession: "Software Engineer",
     email: "john.smith@example.com",
     source_ids: ["person/123", "person/456", "person/789"],
     sources: ["database1", "database2", "web"]
   }
   ```

2. **Conflict resolution**:

   ```
   Source 1: {name: "Albert Einstein", birthDate: "1879-03-14", deathDate: "1955-04-18"}
   Source 2: {name: "Albert Einstein", birthDate: "1879-03-14", deathDate: "1955-04-17"}

   Resolution options:

   a) Majority voting (if more sources):
   {
     name: "Albert Einstein",
     birthDate: "1879-03-14",
     deathDate: "1955-04-18",  // Selected based on majority
     deathDate_sources: {
       "source1": "1955-04-18",
       "source2": "1955-04-17"
     }
   }

   b) Source reliability weighting:
   {
     name: "Albert Einstein",
     birthDate: "1879-03-14",
     deathDate: "1955-04-18",  // Selected based on source reliability
     deathDate_sources: {
       "source1": {"value": "1955-04-18", "reliability": 0.9},
       "source2": {"value": "1955-04-17", "reliability": 0.7}
     }
   }
   ```

3. **Temporal versioning**:

   ```
   Source 1 (2020): {company: "TechCorp", headquarters: "New York", employees: 1200}
   Source 2 (2022): {company: "TechCorp", headquarters: "Austin", employees: 1500}

   Temporal fusion:
   {
     id: "kg:company:C5678",
     name: "TechCorp",
     headquarters: [
       {value: "New York", from: "2000", to: "2021"},
       {value: "Austin", from: "2021", until: null}
     ],
     employees: [
       {value: 1200, timestamp: "2020"},
       {value: 1500, timestamp: "2022"}
     ]
   }
   ```

:::

Knowledge fusion approaches include:

1. **Rule-based fusion**:

   - Explicit rules for combining information
   - Source prioritization hierarchies
   - Domain-specific conflict resolution
   - Advantages: Interpretable, incorporates domain knowledge
   - Disadvantages: Manual rule creation, maintenance challenges

2. **Statistical fusion**:

   - Voting-based methods
   - Probabilistic models
   - Truth discovery algorithms
   - Advantages: Can handle large-scale fusion, quantify certainty
   - Disadvantages: May oversimplify complex conflicts

3. **Learning-based fusion**:
   - Supervised learning for resolving conflicts
   - Embeddings for semantic similarity
   - Reinforcement learning for fusion policies
   - Advantages: Adaptivity, can learn from examples
   - Disadvantages: Requires training data, less transparent

::: {#exm-fusion-implementation}

## Knowledge fusion implementation

1. **Rule-based fusion implementation**:

   ```python
   def rule_based_fusion(entity_references, attribute_prioritization=None):
       """
       Fuse entity information using rule-based approach

       entity_references: List of dictionaries containing entity data from different sources
       attribute_prioritization: Optional dictionary specifying source priorities per attribute
       """
       if not entity_references:
           return None

       # Start with an empty fused entity
       fused_entity = {
           "sources": [],
           "source_details": {}
       }

       # Track all attributes encountered
       all_attributes = set()
       for entity in entity_references:
           all_attributes.update(entity.keys())

       # Remove metadata fields
       for meta_field in ["source", "source_id", "confidence"]:
           if meta_field in all_attributes:
               all_attributes.remove(meta_field)

       # Process each attribute
       for attribute in all_attributes:
           # Collect all values for this attribute from different sources
           attribute_values = []

           for entity in entity_references:
               if attribute in entity:
                   source = entity.get("source", "unknown")
                   source_id = entity.get("source_id", "unknown")
                   confidence = entity.get("confidence", 1.0)

                   attribute_values.append({
                       "value": entity[attribute],
                       "source": source,
                       "source_id": source_id,
                       "confidence": confidence
                   })

           # Skip if no values found
           if not attribute_values:
               continue

           # Apply fusion rules
           fused_value = None
           value_sources = []

           # Check if we have prioritization rules for this attribute
           if attribute_prioritization and attribute in attribute_prioritization:
               # Sort by priority
               priorities = attribute_prioritization[attribute]
               attribute_values.sort(
                   key=lambda x: priorities.index(x["source"])
                   if x["source"] in priorities else len(priorities)
               )
               # Take the highest priority value
               fused_value = attribute_values[0]["value"]
               value_sources = [attribute_values[0]]

           # Apply attribute-specific rules
           elif attribute in ["id", "name", "identifier"]:
               # For identifying attributes, use the most common value
               value_counts = {}
               for item in attribute_values:
                   value = str(item["value"])
                   if value not in value_counts:
                       value_counts[value] = []
                   value_counts[value].append(item)

               # Find the most common value
               most_common = max(value_counts.items(), key=lambda x: len(x[1]))
               fused_value = most_common[0]
               value_sources = most_common[1]

           elif attribute in ["birthDate", "startDate", "creationDate"]:
               # For date attributes, select the most precise date format
               # or the most commonly occurring value
               date_values = {}
               for item in attribute_values:
                   value = str(item["value"])
                   if value not in date_values:
                       date_values[value] = []
                   date_values[value].append(item)

               # Find the most common value
               most_common = max(date_values.items(), key=lambda x: len(x[1]))
               fused_value = most_common[0]
               value_sources = most_common[1]

           elif isinstance(attribute_values[0]["value"], (int, float)):
               # For numeric attributes, use statistical measures
               # Options: mean, median, max (most recent), etc.
               values = [item["value"] for item in attribute_values]
               confidences = [item["confidence"] for item in attribute_values]

               # Weighted average based on confidence
               if sum(confidences) > 0:
                   fused_value = sum(v * c for v, c in zip(values, confidences)) / sum(confidences)
               else:
                   fused_value = sum(values) / len(values)

               value_sources = attribute_values

           elif isinstance(attribute_values[0]["value"], list):
               # For list attributes, combine unique values
               combined_list = []
               for item in attribute_values:
                   if isinstance(item["value"], list):
                       combined_list.extend(item["value"])
                   else:
                       combined_list.append(item["value"])

               # Remove duplicates while preserving order
               seen = set()
               fused_value = [x for x in combined_list if not (x in seen or seen.add(x))]
               value_sources = attribute_values

           else:
               # Default: use the value with highest confidence
               attribute_values.sort(key=lambda x: x["confidence"], reverse=True)
               fused_value = attribute_values[0]["value"]
               value_sources = [attribute_values[0]]

           # Add the fused value to the result
           fused_entity[attribute] = fused_value

           # Track source details
           fused_entity["source_details"][attribute] = value_sources

           # Add source to overall sources list
           for source in value_sources:
               if source["source"] not in fused_entity["sources"]:
                   fused_entity["sources"].append(source["source"])

       return fused_entity
   ```

2. **Statistical fusion with truth discovery**:

   ```python
   def truth_discovery_fusion(entity_references, max_iterations=20, convergence_threshold=0.001):
       """
       Fuse entity information using truth discovery algorithm

       entity_references: List of dictionaries containing entity data from different sources
       max_iterations: Maximum number of iterations for convergence
       convergence_threshold: Threshold for stopping iterations
       """
       if not entity_references:
           return None

       # Extract all sources and attributes
       sources = set()
       attributes = set()

       for entity in entity_references:
           sources.add(entity.get("source", "unknown"))
           for key in entity.keys():
               if key not in ["source", "source_id", "confidence"]:
                   attributes.add(key)

       # Initialize source weights (reliability)
       source_weights = {source: 1.0 for source in sources}

       # Initialize true values with a simple vote
       true_values = {}
       for attribute in attributes:
           value_counts = {}
           for entity in entity_references:
               if attribute in entity:
                   value = str(entity[attribute])
                   value_counts[value] = value_counts.get(value, 0) + 1

           if value_counts:
               most_common = max(value_counts.items(), key=lambda x: x[1])
               true_values[attribute] = most_common[0]

       # Iterative algorithm
       for iteration in range(max_iterations):
           # Store old weights for convergence check
           old_weights = source_weights.copy()

           # Step 1: Compute source weights based on agreement with current true values
           source_error_sum = {source: 0.0 for source in sources}
           source_provided_count = {source: 0 for source in sources}

           for entity in entity_references:
               source = entity.get("source", "unknown")
               for attribute in attributes:
                   if attribute in entity and attribute in true_values:
                       # Compute error (0 if agrees with true value, 1 otherwise)
                       error = 0.0 if str(entity[attribute]) == true_values[attribute] else 1.0
                       source_error_sum[source] += error
                       source_provided_count[source] += 1

           # Update source weights
           for source in sources:
               if source_provided_count[source] > 0:
                   avg_error = source_error_sum[source] / source_provided_count[source]
                   # Weight is inversely proportional to error
                   source_weights[source] = 1.0 / (avg_error + 0.00001)  # Avoid division by zero

           # Normalize weights
           weight_sum = sum(source_weights.values())
           if weight_sum > 0:
               for source in sources:
                   source_weights[source] /= weight_sum

           # Step 2: Update true values based on weighted voting
           new_true_values = {}
           for attribute in attributes:
               weighted_votes = {}
               for entity in entity_references:
                   if attribute in entity:
                       source = entity.get("source", "unknown")
                       value = str(entity[attribute])
                       if value not in weighted_votes:
                           weighted_votes[value] = 0
                       weighted_votes[value] += source_weights[source]

               if weighted_votes:
                   best_value = max(weighted_votes.items(), key=lambda x: x[1])
                   new_true_values[attribute] = best_value[0]

           # Update true values
           true_values = new_true_values

           # Check for convergence
           weight_diff = sum(abs(source_weights[s] - old_weights[s]) for s in sources)
           if weight_diff < convergence_threshold:
               break

       # Construct fused entity
       fused_entity = true_values.copy()

       # Add source information
       fused_entity["sources"] = list(sources)
       fused_entity["source_weights"] = source_weights

       # Add detailed provenance
       source_details = {}
       for attribute in attributes:
           if attribute in true_values:
               attribute_sources = []
               for entity in entity_references:
                   if attribute in entity and str(entity[attribute]) == true_values[attribute]:
                       source = entity.get("source", "unknown")
                       source_id = entity.get("source_id", "unknown")
                       attribute_sources.append({
                           "source": source,
                           "source_id": source_id,
                           "weight": source_weights[source]
                       })
               source_details[attribute] = attribute_sources

       fused_entity["source_details"] = source_details

       return fused_entity
   ```

3. **Temporal conflict resolution**:
   ```python
   def temporal_fusion(entity_references):
       """
       Fuse entity information with temporal awareness

       entity_references: List of dictionaries containing entity data from different sources,
                         each with a timestamp
       """
       if not entity_references:
           return None

       # Sort entity references by timestamp
       sorted_refs = sorted(
           entity_references,
           key=lambda x: x.get("timestamp", "0000-00-00")
       )

       # Extract all attributes (excluding metadata)
       attributes = set()
       for entity in sorted_refs:
           for key in entity.keys():
               if key not in ["source", "source_id", "confidence", "timestamp"]:
                   attributes.add(key)

       # Track the evolution of attribute values over time
       attribute_history = {}
       for attribute in attributes:
           attribute_history[attribute] = []

       # Current state of the entity
       current_state = {}

       # Process entity references in chronological order
       for entity in sorted_refs:
           timestamp = entity.get("timestamp", "unknown")
           source = entity.get("source", "unknown")

           for attribute in attributes:
               if attribute in entity:
                   current_value = current_state.get(attribute)
                   new_value = entity[attribute]

                   # Check if value has changed
                   if current_value != new_value:
                       # If there's an existing value, close its time period
                       if attribute in current_state:
                           attribute_history[attribute][-1]["to"] = timestamp

                       # Add new value
                       attribute_history[attribute].append({
                           "value": new_value,
                           "from": timestamp,
                           "to": None,  # Still current
                           "source": source
                       })

                       # Update current state
                       current_state[attribute] = new_value

       # Construct fused entity with temporal information
       fused_entity = {
           "current": current_state,
           "history": attribute_history,
           "sources": list(set(e.get("source", "unknown") for e in sorted_refs))
       }

       return fused_entity
   ```

:::

Challenges in knowledge fusion include:

1. **Conflicting information**: Determining truth when sources disagree
2. **Missing temporal context**: Resolving conflicts without clear timestamps
3. **Source reliability assessment**: Objectively evaluating source credibility
4. **Interdependent attributes**: Handling cases where attributes should be jointly fused
5. **Schema heterogeneity**: Fusing information across different conceptual models

### Ontology-based integration

Ontology-based integration uses formal ontologies to guide and structure the integration process:

::: {#def-ontology-integration}

## Ontology-based integration

**Ontology-based integration** uses ontologies to guide the process of combining information from multiple sources. Key aspects include:

1. **Semantic anchoring**: Using ontologies as a reference model for integration
2. **Inference-based integration**: Leveraging logical inference to derive implicit connections
3. **Constraint validation**: Ensuring integrated data satisfies ontological constraints
4. **Cross-ontology mapping**: Connecting concepts across different domain ontologies
5. **Upper ontology alignment**: Using top-level ontologies for consistent integration

:::

::: {#exm-ontology-integration}

## Ontology-based integration examples

1. **Integration through shared ontology**:

   ```
   Source 1 schema:
   - Class: Person (properties: name, birthDate, gender)
   - Class: Publication (properties: title, year, authors)

   Source 2 schema:
   - Class: Author (properties: fullName, dob, gender)
   - Class: Paper (properties: paperTitle, publicationYear, writtenBy)

   Shared ontology:
   - Class: foaf:Person (properties: foaf:name, bio:birth, foaf:gender)
   - Class: bibo:Document (properties: dcterms:title, dcterms:date, dcterms:creator)

   Integration:
   - Person maps to foaf:Person
     - name → foaf:name
     - birthDate → bio:birth
     - gender → foaf:gender

   - Author maps to foaf:Person
     - fullName → foaf:name
     - dob → bio:birth
     - gender → foaf:gender

   - Publication maps to bibo:Document
     - title → dcterms:title
     - year → dcterms:date
     - authors → dcterms:creator

   - Paper maps to bibo:Document
     - paperTitle → dcterms:title
     - publicationYear → dcterms:date
     - writtenBy → dcterms:creator
   ```

2. **Integration with ontological constraints**:

   ```
   Ontology constraints:
   - Class: MedicalDoctor (subClassOf: HealthcareProfessional)
     - Constraint: must have at least one medicalSpecialty
     - Constraint: must have a medicalLicense

   Source 1 data:
   {
     "id": "doc123",
     "type": "Physician",
     "name": "Dr. Smith",
     "specialty": "Cardiology",
     "license": "MD12345"
   }

   Source 2 data:
   {
     "id": "physician456",
     "type": "Doctor",
     "name": "Jane Smith, MD",
     "practices": ["Heart Surgery", "Vascular Medicine"]
   }

   Integrated result with constraint validation:
   {
     "id": "kg:MedicalDoctor:MD789",
     "type": "MedicalDoctor",
     "name": "Dr. Jane Smith",
     "medicalSpecialty": ["Cardiology", "Heart Surgery", "Vascular Medicine"],
     "medicalLicense": "MD12345",
     "validation": {
       "compliant": true,
       "missing": [],
       "violations": []
     }
   }
   ```

3. **Integration with inference**:

   ```
   Ontology axioms:
   - Cardiologist SubClassOf: MedicalSpecialist
   - Cardiology SubClassOf: MedicalSpecialty
   - hasSpecialty Domain: MedicalPractitioner Range: MedicalSpecialty
   - practices SubPropertyOf: hasSpecialty
   - Cardiologist EquivalentTo: MedicalPractitioner and (hasSpecialty some Cardiology)

   Source data:
   {
     "id": "doc789",
     "type": "Physician",
     "name": "Dr. Johnson",
     "practices": "Cardiology"
   }

   Integrated result with inference:
   {
     "id": "kg:MedicalPractitioner:MP101",
     "type": ["MedicalPractitioner", "Cardiologist"],  // Inferred type
     "name": "Dr. Johnson",
     "hasSpecialty": "Cardiology",  // Original: practices
     "inferred": true
   }
   ```

:::

Ontology-based integration approaches include:

1. **Global-as-view (GAV)**:

   - Defining the global schema (ontology) as a view over source schemas
   - Generating integrated data by evaluating views over source data
   - Advantages: Straightforward query processing, conceptual clarity
   - Disadvantages: Sensitive to changes in sources, complex for many sources

2. **Local-as-view (LAV)**:

   - Defining source schemas as views over a global ontology
   - Rewriting queries against the global schema into source-specific queries
   - Advantages: More resilient to source changes, easier to add new sources
   - Disadvantages: More complex query processing, challenging view definitions

3. **Hybrid approaches**:
   - Global-local-as-view (GLAV) combining aspects of both
   - Peer-to-peer semantic mapping networks
   - Advantages: Flexibility, scalability for complex integration scenarios
   - Disadvantages: Increased complexity, harder to guarantee global consistency

::: {#exm-ontology-integration-implementation}

## Ontology-based integration implementation

1. **Integration using OWL reasoning**:

   ```python
   from owlready2 import *
   import rdflib

   def ontology_based_integration(source_data_list, mapping_list, ontology_file):
       """
       Integrate multiple data sources using ontology-based approach

       source_data_list: List of data from different sources
       mapping_list: List of mappings from source schemas to ontology
       ontology_file: Path to the shared ontology file (OWL)
       """
       # Load the ontology
       onto = get_ontology(ontology_file).load()

       # Create a world for reasoning
       world = onto.world

       # Process each source data using its mapping
       for source_data, mapping in zip(source_data_list, mapping_list):
           integrate_source(source_data, mapping, onto, world)

       # Run the reasoner to apply inference
       with onto:
           sync_reasoner_pellet(world, infer_property_values=True)

       # Extract integrated data
       integrated_data = extract_integrated_data(onto)

       return integrated_data

   def integrate_source(source_data, mapping, onto, world):
       """Integrate a single source into the ontology"""
       # Process each entity in the source data
       for entity in source_data:
           # Map source entity type to ontology class
           if "type" in entity and entity["type"] in mapping["class_mappings"]:
               ontology_class_name = mapping["class_mappings"][entity["type"]]
               ontology_class = getattr(onto, ontology_class_name)

               # Create instance with a unique ID
               if "id" in entity:
                   instance_id = f"{ontology_class_name}_{entity['id']}"
               else:
                   instance_id = f"{ontology_class_name}_{len(list(ontology_class.instances()))}"

               instance = ontology_class(instance_id)

               # Map and set properties
               for prop_name, prop_value in entity.items():
                   if prop_name in mapping["property_mappings"]:
                       onto_prop_name = mapping["property_mappings"][prop_name]
                       onto_prop = getattr(onto, onto_prop_name)

                       # Handle different property types
                       if onto_prop.range in [str, int, float, bool]:
                           # Data property
                           setattr(instance, onto_prop_name, prop_value)
                       else:
                           # Object property - handle reference
                           # This assumes the referenced entity already exists or will be created
                           # In practice, this would need more sophisticated reference resolution
                           referenced_instance = find_or_create_reference(
                               prop_value, onto, mapping
                           )
                           if referenced_instance:
                               setattr(instance, onto_prop_name, referenced_instance)

   def find_or_create_reference(ref_value, onto, mapping):
       """Find or create a referenced entity"""
       # This is a simplified implementation
       # Real systems would need more sophisticated entity resolution

       # If the reference is a string identifier
       if isinstance(ref_value, str):
           # Try to find existing instance with this ID
           for cls in onto.classes():
               for instance in cls.instances():
                   if instance.name == ref_value:
                       return instance

           # Not found - could create a stub instance
           return None

       # If the reference is a dictionary (nested entity)
       elif isinstance(ref_value, dict):
           if "type" in ref_value and ref_value["type"] in mapping["class_mappings"]:
               ontology_class_name = mapping["class_mappings"][ref_value["type"]]
               ontology_class = getattr(onto, ontology_class_name)

               if "id" in ref_value:
                   instance_id = f"{ontology_class_name}_{ref_value['id']}"
               else:
                   instance_id = f"{ontology_class_name}_{len(list(ontology_class.instances()))}"

               # Check if instance already exists
               for instance in ontology_class.instances():
                   if instance.name == instance_id:
                       return instance

               # Create new instance
               return ontology_class(instance_id)

       return None

   def extract_integrated_data(onto):
       """Extract the integrated data from the ontology"""
       integrated_data = {}

       # Process each class in the ontology
       for cls in onto.classes():
           class_name = cls.name
           integrated_data[class_name] = []

           # Process each instance of the class
           for instance in cls.instances():
               instance_data = {"id": instance.name}

               # Get all property values
               for prop in onto.object_properties() + onto.data_properties():
                   values = getattr(instance, prop.name)
                   if values:
                       # Convert to appropriate representation
                       if isinstance(values, list):
                           # Multiple values
                           instance_data[prop.name] = [
                               v.name if hasattr(v, 'name') else v
                               for v in values
                           ]
                       else:
                           # Single value
                           instance_data[prop.name] = (
                               values.name if hasattr(values, 'name') else values
                           )

               integrated_data[class_name].append(instance_data)

       return integrated_data
   ```

2. **SPARQL-based ontology integration**:

   ```python
   from rdflib import Graph, URIRef, Literal, Namespace
   from rdflib.namespace import RDF, RDFS, OWL
   import json

   def sparql_based_integration(source_files, mapping_files, ontology_file):
       """
       Integrate multiple RDF sources using SPARQL and a shared ontology

       source_files: List of RDF files containing source data
       mapping_files: List of SPARQL CONSTRUCT files for mapping
       ontology_file: RDF file containing the shared ontology
       """
       # Load shared ontology
       integrated_graph = Graph()
       integrated_graph.parse(ontology_file)

       # Load each source and apply its mapping
       for source_file, mapping_file in zip(source_files, mapping_files):
           # Load source data
           source_graph = Graph()
           source_graph.parse(source_file)

           # Load mapping (SPARQL CONSTRUCT query)
           with open(mapping_file, 'r') as f:
               construct_query = f.read()

           # Apply mapping
           mapping_result = source_graph.query(construct_query)

           # Add mapped triples to integrated graph
           for triple in mapping_result:
               integrated_graph.add(triple)

       # Apply reasoning to integrate further
       # This is a placeholder - actual reasoning would depend on the specific reasoner
       apply_owl_reasoning(integrated_graph)

       # Validate against ontology constraints
       validation_results = validate_constraints(integrated_graph)

       # Extract integrated data in a structured format
       integrated_data = extract_structured_data(integrated_graph)

       return {
           "integrated_graph": integrated_graph,
           "validation": validation_results,
           "structured_data": integrated_data
       }

   def apply_owl_reasoning(graph):
       """Apply OWL reasoning to the graph"""
       # In practice, this would use a specific OWL reasoner
       # like HermiT, Pellet, or FaCT++

       # Placeholder implementation - in a real system, you would use
       # a reasoning library or service

       # Example: Apply RDFS inference
       rdfs_rules = """
       PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

       # Subclass transitivity
       CONSTRUCT {
           ?x rdfs:subClassOf ?z .
       }
       WHERE {
           ?x rdfs:subClassOf ?y .
           ?y rdfs:subClassOf ?z .
       }
       """

       inference_results = graph.query(rdfs_rules)
       for triple in inference_results:
           graph.add(triple)

       # More inference rules would be added here

       return graph

   def validate_constraints(graph):
       """Validate the integrated graph against ontology constraints"""
       validation_results = {
           "valid": True,
           "violations": []
       }

       # Example constraint check: Every Person must have a name
       person_name_check = """
       PREFIX ex: <http://example.org/ontology#>
       PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>

       SELECT ?person
       WHERE {
           ?person rdf:type ex:Person .
           FILTER NOT EXISTS { ?person ex:name ?name }
       }
       """

       results = graph.query(person_name_check)
       for row in results:
           validation_results["valid"] = False
           validation_results["violations"].append({
               "entity": str(row[0]),
               "constraint": "Person must have a name property",
               "type": "missing_required_property"
           })

       # Additional constraint checks would be added here

       return validation_results

   def extract_structured_data(graph):
       """Extract structured data from the RDF graph"""
       structured_data = {}

       # Get all classes in the ontology
       class_query = """
       PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
       PREFIX owl: <http://www.w3.org/2002/07/owl#>

       SELECT DISTINCT ?class
       WHERE {
           { ?class a owl:Class } UNION { ?class a rdfs:Class }
           FILTER(?class != owl:Class && ?class != rdfs:Class)
       }
       """

       for row in graph.query(class_query):
           class_uri = row[0]
           class_name = class_uri.split('#')[-1] if '#' in class_uri else class_uri.split('/')[-1]
           structured_data[class_name] = []

           # Get all instances of this class
           instance_query = f"""
           SELECT DISTINCT ?instance
           WHERE {{
               ?instance a <{class_uri}> .
           }}
           """

           for instance_row in graph.query(instance_query):
               instance_uri = instance_row[0]
               instance_data = {"uri": str(instance_uri)}

               # Get all properties of this instance
               property_query = f"""
               SELECT ?property ?value
               WHERE {{
                   <{instance_uri}> ?property ?value .
                   FILTER(?property != rdf:type)
               }}
               """

               for prop_row in graph.query(property_query):
                   prop_uri = prop_row[0]
                   value = prop_row[1]

                   prop_name = prop_uri.split('#')[-1] if '#' in prop_uri else prop_uri.split('/')[-1]

                   # Handle literal vs. URI values differently
                   if isinstance(value, Literal):
                       instance_data[prop_name] = value.value
                   else:
                       instance_data[prop_name] = str(value)

               structured_data[class_name].append(instance_data)

       return structured_data
   ```

3. **Modular ontology integration**:

   ```python
   def modular_ontology_integration(domain_ontologies, data_sources, upper_ontology=None):
       """
       Integrate multiple domain ontologies and their data

       domain_ontologies: List of domain-specific ontologies
       data_sources: List of data sources corresponding to ontologies
       upper_ontology: Optional upper ontology for alignment
       """
       # Create integrated ontology
       integrated_ontology = Graph()

       # Add upper ontology if provided
       if upper_ontology:
           integrated_ontology.parse(upper_ontology)

       # Process each domain ontology
       ontology_mappings = []
       for i, (ontology_file, mapping_file) in enumerate(domain_ontologies):
           # Load domain ontology
           domain_graph = Graph()
           domain_graph.parse(ontology_file)

           # Load alignment/mapping to upper ontology
           with open(mapping_file, 'r') as f:
               mapping_data = json.load(f)

           # Apply mappings
           mapped_triples = apply_ontology_mapping(domain_graph, mapping_data)

           # Add mapped ontology to integrated ontology
           for triple in mapped_triples:
               integrated_ontology.add(triple)

           # Store mappings for data integration
           ontology_mappings.append(mapping_data)

       # Integrate data sources based on ontology mappings
       integrated_data = []
       for data_source, mapping in zip(data_sources, ontology_mappings):
           # Transform data according to mapping
           transformed_data = transform_data_with_mapping(data_source, mapping)
           integrated_data.extend(transformed_data)

       return {
           "ontology": integrated_ontology,
           "data": integrated_data
       }

   def apply_ontology_mapping(domain_graph, mapping_data):
       """Apply mappings from a domain ontology to an upper ontology"""
       result_graph = Graph()

       # Copy all original triples
       for triple in domain_graph:
           result_graph.add(triple)

       # Apply class mappings
       for class_mapping in mapping_data.get("class_mappings", []):
           domain_class = URIRef(class_mapping["domain_class"])
           upper_class = URIRef(class_mapping["upper_class"])
           relation = URIRef(class_mapping.get("relation", str(RDFS.subClassOf)))

           # Add mapping triple
           result_graph.add((domain_class, relation, upper_class))

       # Apply property mappings
       for prop_mapping in mapping_data.get("property_mappings", []):
           domain_prop = URIRef(prop_mapping["domain_property"])
           upper_prop = URIRef(prop_mapping["upper_property"])
           relation = URIRef(prop_mapping.get("relation", str(RDFS.subPropertyOf)))

           # Add mapping triple
           result_graph.add((domain_prop, relation, upper_prop))

       # Apply instance mappings
       for instance_mapping in mapping_data.get("instance_mappings", []):
           domain_instance = URIRef(instance_mapping["domain_instance"])
           upper_instance = URIRef(instance_mapping["upper_instance"])
           relation = URIRef(instance_mapping.get("relation", str(OWL.sameAs)))

           # Add mapping triple
           result_graph.add((domain_instance, relation, upper_instance))

       return result_graph

   def transform_data_with_mapping(data_source, mapping_data):
       """Transform data according to ontology mappings"""
       transformed_data = []

       # Load data
       if isinstance(data_source, str):
           # Assume it's a file path
           with open(data_source, 'r') as f:
               source_data = json.load(f)
       else:
           # Already loaded data
           source_data = data_source

       # Get reverse mappings for transformation
       class_map = {m["domain_class"]: m["upper_class"] for m in mapping_data.get("class_mappings", [])}
       property_map = {m["domain_property"]: m["upper_property"] for m in mapping_data.get("property_mappings", [])}

       # Transform each entity
       for entity in source_data:
           transformed_entity = {}

           # Transform type if present
           if "type" in entity and entity["type"] in class_map:
               transformed_entity["type"] = class_map[entity["type"]]
           else:
               transformed_entity["type"] = entity.get("type")

           # Transform ID and basic properties
           transformed_entity["id"] = entity.get("id")
           transformed_entity["source"] = entity.get("source", "unknown")

           # Transform other properties
           for prop, value in entity.items():
               if prop in ["type", "id", "source"]:
                   continue

               if prop in property_map:
                   # Map property name
                   mapped_prop = property_map[prop]
                   transformed_entity[mapped_prop] = value
               else:
                   # Keep original property
                   transformed_entity[prop] = value

           transformed_data.append(transformed_entity)

       return transformed_data
   ```

:::

Challenges in ontology-based integration include:

1. **Ontology complexity**: Balancing expressivity with usability
2. **Reasoning scalability**: Handling large-scale inference over integrated data
3. **Mapping complexity**: Creating and maintaining complex ontology alignments
4. **Inconsistency management**: Resolving conflicts between integrated ontologies
5. **User adoption**: Making ontology-based approaches accessible to data practitioners

## Quality assessment and enrichment

Ensuring the quality of a constructed knowledge graph and enhancing it with additional knowledge is crucial for its usefulness and reliability.

### Knowledge graph quality dimensions

Quality assessment for knowledge graphs spans multiple dimensions:

::: {#def-quality-dimensions}

## Knowledge graph quality dimensions

**Knowledge graph quality dimensions** are aspects by which the quality of a knowledge graph can be evaluated:

1. **Accuracy**: Correctness of the information in the knowledge graph
2. **Completeness**: Coverage of the relevant domain knowledge
3. **Consistency**: Absence of contradictions and adherence to schema constraints
4. **Timeliness**: Currency and freshness of the information
5. **Accessibility**: Ease of accessing and querying the knowledge
6. **Relevance**: Importance of the included information for target applications
7. **Trustworthiness**: Reliability and credibility of the information sources
8. **Verifiability**: Ability to trace information to its original sources

These dimensions can be assessed through various metrics and evaluation techniques.

:::

::: {#exm-quality-dimensions}

## Knowledge graph quality examples

1. **Accuracy assessment**:

   ```
   Entity: Albert Einstein
   Attribute: birthDate
   Value: "1879-03-14"

   Accuracy check:
   - External validation against authoritative sources
   - Multiple source agreement: 8/10 sources confirm (80% agreement)
   - No logical inconsistencies (e.g., not before parent's birth)
   - Format validation passes (valid date format)

   Result: High accuracy (0.95)
   ```

2. **Completeness assessment**:

   ```
   Domain: Scientists
   Schema completeness:
   - Required properties: name, birthDate, field, nationality, notableWorks
   - Optional properties: deathDate, awards, education, influences

   Entity: Marie Curie
   - Present properties: name, birthDate, deathDate, field, nationality, notableWorks, awards
   - Missing properties: education, influences

   Result:
   - Required property completeness: 100%
   - Overall property completeness: 78%
   - Population completeness: Coverage of 85% of Nobel Prize winners in Physics and Chemistry
   ```

3. **Consistency assessment**:

   ```
   Logical consistency check:
   - Schema constraint: A person's death date must be after their birth date
   - Ontological constraint: A person cannot be both a student and a teacher of the same person
   - Type constraint: All values for "population" must be positive integers

   Violations found:
   - Entity "John Smith" has birthDate "1980-01-01" but deathDate "1970-12-31"
   - Entity "London" has population "-5000" (negative value)

   Result: 98.7% of entities satisfy all consistency constraints
   ```

:::

Approaches to quality assessment include:

1. **Manual assessment**:

   - Expert review of knowledge graph content
   - User feedback and error reporting
   - Advantages: High precision, domain expertise
   - Disadvantages: Limited scalability, subjective judgments

2. **Automated assessment**:

   - Rule-based validation using schema constraints
   - Statistical analysis of patterns and outliers
   - Cross-reference checking with external sources
   - Advantages: Scalability, objectivity
   - Disadvantages: Limited semantic understanding

3. **Hybrid approaches**:
   - Automated detection of potential issues
   - Manual review of flagged problems
   - Continuous monitoring with periodic expert validation
   - Advantages: Efficient use of human expertise
   - Disadvantages: Workflow complexity

::: {#exm-quality-assessment-implementation}

## Quality assessment implementation

1. **Accuracy assessment implementation**:

   ```python
   def assess_accuracy(knowledge_graph, reference_sources=None, sample_size=100):
       """
       Assess the accuracy of a knowledge graph

       knowledge_graph: The knowledge graph to assess
       reference_sources: Optional list of trusted external sources
       sample_size: Number of facts to sample for manual validation
       """
       accuracy_metrics = {
           "overall": 0.0,
           "by_relation_type": {},
           "by_entity_type": {},
           "sample_results": []
       }

       # If reference sources are provided, check against them
       if reference_sources:
           # For each entity in the knowledge graph
           correct_facts = 0
           total_checked_facts = 0

           for entity_id, entity_data in knowledge_graph.items():
               entity_type = entity_data.get("type")

               # For each attribute of the entity
               for attribute, value in entity_data.items():
                   if attribute in ["id", "type", "source"]:
                       continue

                   # Check this fact against reference sources
                   fact_correct = check_fact_against_references(
                       entity_id, attribute, value, reference_sources
                   )

                   if fact_correct is not None:
                       total_checked_facts += 1
                       if fact_correct:
                           correct_facts += 1

                       # Track by relation type
                       if attribute not in accuracy_metrics["by_relation_type"]:
                           accuracy_metrics["by_relation_type"][attribute] = {
                               "correct": 0, "total": 0
                           }

                       accuracy_metrics["by_relation_type"][attribute]["total"] += 1
                       if fact_correct:
                           accuracy_metrics["by_relation_type"][attribute]["correct"] += 1

                       # Track by entity type
                       if entity_type:
                           if entity_type not in accuracy_metrics["by_entity_type"]:
                               accuracy_metrics["by_entity_type"][entity_type] = {
                                   "correct": 0, "total": 0
                               }

                           accuracy_metrics["by_entity_type"][entity_type]["total"] += 1
                           if fact_correct:
                               accuracy_metrics["by_entity_type"][entity_type]["correct"] += 1

           # Calculate overall accuracy
           if total_checked_facts > 0:
               accuracy_metrics["overall"] = correct_facts / total_checked_facts

               # Calculate accuracy by relation type
               for rel_type in accuracy_metrics["by_relation_type"]:
                   rel_data = accuracy_metrics["by_relation_type"][rel_type]
                   if rel_data["total"] > 0:
                       rel_data["accuracy"] = rel_data["correct"] / rel_data["total"]

               # Calculate accuracy by entity type
               for entity_type in accuracy_metrics["by_entity_type"]:
                   type_data = accuracy_metrics["by_entity_type"][entity_type]
                   if type_data["total"] > 0:
                       type_data["accuracy"] = type_data["correct"] / type_data["total"]

       # Sample facts for manual validation if needed
       if sample_size > 0:
           sampled_facts = sample_facts_for_validation(knowledge_graph, sample_size)
           accuracy_metrics["sample_results"] = sampled_facts

       return accuracy_metrics

   def check_fact_against_references(entity_id, attribute, value, reference_sources):
       """
       Check a single fact against reference sources
       Returns True if correct, False if incorrect, None if not found
       """
       for source in reference_sources:
           # This would be implemented differently depending on the type of reference
           # For example, API calls to external knowledge bases, database queries, etc.
           reference_value = source.lookup_fact(entity_id, attribute)

           if reference_value is not None:
               # Compare values (allowing for slight variation)
               return values_match(value, reference_value)

       # Not found in any reference
       return None

   def values_match(value1, value2, tolerance=0.1):
       """Compare two values with tolerance for different formats/styles"""
       if value1 == value2:
           return True

       # For numeric values, check if they're close
       if isinstance(value1, (int, float)) and isinstance(value2, (int, float)):
           diff = abs(value1 - value2)
           avg = (abs(value1) + abs(value2)) / 2

           if avg == 0:
               return diff == 0

           return diff / avg <= tolerance

       # For string values, check similarity
       if isinstance(value1, str) and isinstance(value2, str):
           # Simple normalization
           v1 = value1.lower().strip()
           v2 = value2.lower().strip()

           if v1 == v2:
               return True

           # Check if one is contained in the other
           if v1 in v2 or v2 in v1:
               return True

           # Could add more sophisticated string similarity here
           # Such as Levenshtein distance, Jaccard similarity, etc.

       # For lists, check if they contain similar elements
       if isinstance(value1, list) and isinstance(value2, list):
           # Check if at least 70% of elements match
           matches = 0
           for v1 in value1:
               if any(values_match(v1, v2) for v2 in value2):
                   matches += 1

           if len(value1) > 0:
               match_ratio = matches / len(value1)
               return match_ratio >= 0.7

       return False

   def sample_facts_for_validation(knowledge_graph, sample_size):
       """Sample facts from the knowledge graph for manual validation"""
       all_facts = []

       # Collect all facts
       for entity_id, entity_data in knowledge_graph.items():
           for attribute, value in entity_data.items():
               if attribute in ["id", "type", "source"]:
                   continue

               all_facts.append({
                   "entity_id": entity_id,
                   "entity_name": entity_data.get("name", entity_id),
                   "attribute": attribute,
                   "value": value,
                   "validation_status": "pending"  # To be filled in manually
               })

       # Randomly sample facts
       import random
       if sample_size < len(all_facts):
           sampled_facts = random.sample(all_facts, sample_size)
       else:
           sampled_facts = all_facts

       return sampled_facts
   ```

2. **Completeness assessment implementation**:

   ```python
   def assess_completeness(knowledge_graph, schema=None, reference_population=None):
       """
       Assess the completeness of a knowledge graph

       knowledge_graph: The knowledge graph to assess
       schema: Optional schema defining required and optional properties
       reference_population: Optional reference population for coverage assessment
       """
       completeness_metrics = {
           "schema_completeness": {},
           "population_completeness": {},
           "overall_completeness": 0.0
       }

       # Schema completeness
       if schema:
           by_entity_type = {}

           # Process each entity
           for entity_id, entity_data in knowledge_graph.items():
               entity_type = entity_data.get("type")

               if entity_type and entity_type in schema:
                   # Initialize metrics for this type if first encounter
                   if entity_type not in by_entity_type:
                       type_schema = schema[entity_type]
                       required_props = type_schema.get("required_properties", [])
                       optional_props = type_schema.get("optional_properties", [])
                       all_props = required_props + optional_props

                       by_entity_type[entity_type] = {
                           "entity_count": 0,
                           "required_completeness": {
                               "total_required": len(required_props) * 0,  # Will be updated
                               "total_present": 0
                           },
                           "overall_completeness": {
                               "total_possible": len(all_props) * 0,  # Will be updated
                               "total_present": 0
                           },
                           "by_property": {prop: {"present": 0, "total": 0} for prop in all_props}
                       }

                   # Update metrics for this entity
                   by_entity_type[entity_type]["entity_count"] += 1

                   # Get schema details
                   type_schema = schema[entity_type]
                   required_props = type_schema.get("required_properties", [])
                   optional_props = type_schema.get("optional_properties", [])
                   all_props = required_props + optional_props

                   # Update total counts
                   by_entity_type[entity_type]["required_completeness"]["total_required"] += len(required_props)
                   by_entity_type[entity_type]["overall_completeness"]["total_possible"] += len(all_props)

                   # Check each required property
                   for prop in required_props:
                       by_entity_type[entity_type]["by_property"][prop]["total"] += 1
                       if prop in entity_data:
                           by_entity_type[entity_type]["required_completeness"]["total_present"] += 1
                           by_entity_type[entity_type]["overall_completeness"]["total_present"] += 1
                           by_entity_type[entity_type]["by_property"][prop]["present"] += 1

                   # Check each optional property
                   for prop in optional_props:
                       by_entity_type[entity_type]["by_property"][prop]["total"] += 1
                       if prop in entity_data:
                           by_entity_type[entity_type]["overall_completeness"]["total_present"] += 1
                           by_entity_type[entity_type]["by_property"][prop]["present"] += 1

           # Calculate completeness ratios
           for entity_type, metrics in by_entity_type.items():
               # Required properties completeness
               required = metrics["required_completeness"]
               if required["total_required"] > 0:
                   required["completeness_ratio"] = required["total_present"] / required["total_required"]
               else:
                   required["completeness_ratio"] = 1.0  # No required properties

               # Overall properties completeness
               overall = metrics["overall_completeness"]
               if overall["total_possible"] > 0:
                   overall["completeness_ratio"] = overall["total_present"] / overall["total_possible"]
               else:
                   overall["completeness_ratio"] = 1.0  # No properties defined

               # Per-property completeness
               for prop, prop_metrics in metrics["by_property"].items():
                   if prop_metrics["total"] > 0:
                       prop_metrics["completeness_ratio"] = prop_metrics["present"] / prop_metrics["total"]
                   else:
                       prop_metrics["completeness_ratio"] = 1.0

           completeness_metrics["schema_completeness"] = by_entity_type

       # Population completeness
       if reference_population:
           population_coverage = {}

           for entity_type, reference_entities in reference_population.items():
               # Count how many reference entities are in the knowledge graph
               covered_count = 0

               for ref_entity in reference_entities:
                   ref_id = ref_entity.get("id")
                   ref_name = ref_entity.get("name")

                   # Check if this reference entity exists in the knowledge graph
                   entity_found = False

                   # Check by ID first
                   if ref_id and ref_id in knowledge_graph:
                       entity_found = True
                   else:
                       # Check by name if ID not found
                       for entity_id, entity_data in knowledge_graph.items():
                           if entity_data.get("type") == entity_type and entity_data.get("name") == ref_name:
                               entity_found = True
                               break

                   if entity_found:
                       covered_count += 1

               # Calculate coverage ratio
               coverage_ratio = covered_count / len(reference_entities) if reference_entities else 0

               population_coverage[entity_type] = {
                   "reference_count": len(reference_entities),
                   "covered_count": covered_count,
                   "coverage_ratio": coverage_ratio
               }

           completeness_metrics["population_completeness"] = population_coverage

       # Calculate overall completeness (simplified)
       # This could be a weighted combination of schema and population completeness
       overall_scores = []

       # Add schema completeness scores
       for entity_type, metrics in completeness_metrics.get("schema_completeness", {}).items():
           overall_scores.append(metrics["overall_completeness"]["completeness_ratio"])

       # Add population completeness scores
       for entity_type, metrics in completeness_metrics.get("population_completeness", {}).items():
           overall_scores.append(metrics["coverage_ratio"])

       # Calculate overall score as average
       if overall_scores:
           completeness_metrics["overall_completeness"] = sum(overall_scores) / len(overall_scores)

       return completeness_metrics
   ```

3. **Consistency assessment implementation**:

   ```python
   def assess_consistency(knowledge_graph, constraints=None, ontology=None):
       """
       Assess the consistency of a knowledge graph

       knowledge_graph: The knowledge graph to assess
       constraints: Optional list of consistency constraints
       ontology: Optional ontology for semantic constraints
       """
       consistency_metrics = {
           "overall_consistency": 1.0,
           "violations": [],
           "constraint_satisfaction": {}
       }

       total_facts = 0
       consistent_facts = 0

       # If constraints are provided, check against them
       if constraints:
           # Initialize constraint satisfaction metrics
           for constraint in constraints:
               constraint_id = constraint.get("id", f"constraint_{len(consistency_metrics['constraint_satisfaction'])}")
               consistency_metrics["constraint_satisfaction"][constraint_id] = {
                   "description": constraint.get("description", ""),
                   "total_applicable": 0,
                   "total_satisfied": 0,
                   "violations": []
               }

           # Check each entity against constraints
           for entity_id, entity_data in knowledge_graph.items():
               entity_type = entity_data.get("type")

               # For each constraint
               for constraint in constraints:
                   constraint_id = constraint.get("id", f"constraint_{len(consistency_metrics['constraint_satisfaction'])}")
                   constraint_type = constraint.get("type")

                   # Skip if constraint doesn't apply to this entity type
                   applicable_types = constraint.get("applicable_types", [])
                   if applicable_types and entity_type not in applicable_types:
                       continue

                   # Mark this constraint as applicable for this entity
                   consistency_metrics["constraint_satisfaction"][constraint_id]["total_applicable"] += 1

                   # Check constraint based on type
                   if constraint_type == "property_type":
                       # Check if property has the correct data type
                       property_name = constraint.get("property")
                       expected_type = constraint.get("expected_type")

                       if property_name in entity_data:
                           value = entity_data[property_name]
                           total_facts += 1

                           type_correct = check_property_type(value, expected_type)
                           if type_correct:
                               consistent_facts += 1
                               consistency_metrics["constraint_satisfaction"][constraint_id]["total_satisfied"] += 1
                           else:
                               violation = {
                                   "entity_id": entity_id,
                                   "constraint_id": constraint_id,
                                   "property": property_name,
                                   "value": value,
                                   "expected_type": expected_type,
                                   "actual_type": type(value).__name__
                               }
                               consistency_metrics["violations"].append(violation)
                               consistency_metrics["constraint_satisfaction"][constraint_id]["violations"].append(violation)

                   elif constraint_type == "cardinality":
                       # Check if property cardinality is within bounds
                       property_name = constraint.get("property")
                       min_count = constraint.get("min_count", 0)
                       max_count = constraint.get("max_count")

                       if property_name in entity_data:
                           value = entity_data[property_name]
                           total_facts += 1

                           # Convert to list if not already
                           value_list = value if isinstance(value, list) else [value]
                           count = len(value_list)

                           cardinality_valid = count >= min_count
                           if max_count is not None:
                               cardinality_valid = cardinality_valid and count <= max_count

                           if cardinality_valid:
                               consistent_facts += 1
                               consistency_metrics["constraint_satisfaction"][constraint_id]["total_satisfied"] += 1
                           else:
                               violation = {
                                   "entity_id": entity_id,
                                   "constraint_id": constraint_id,
                                   "property": property_name,
                                   "actual_count": count,
                                   "min_count": min_count,
                                   "max_count": max_count
                               }
                               consistency_metrics["violations"].append(violation)
                               consistency_metrics["constraint_satisfaction"][constraint_id]["violations"].append(violation)

                   elif constraint_type == "value_range":
                       # Check if numeric property is within specified range
                       property_name = constraint.get("property")
                       min_value = constraint.get("min_value")
                       max_value = constraint.get("max_value")

                       if property_name in entity_data:
                           value = entity_data[property_name]
                           total_facts += 1

                           if isinstance(value, (int, float)):
                               range_valid = True
                               if min_value is not None:
                                   range_valid = range_valid and value >= min_value
                               if max_value is not None:
                                   range_valid = range_valid and value <= max_value

                               if range_valid:
                                   consistent_facts += 1
                                   consistency_metrics["constraint_satisfaction"][constraint_id]["total_satisfied"] += 1
                               else:
                                   violation = {
                                       "entity_id": entity_id,
                                       "constraint_id": constraint_id,
                                       "property": property_name,
                                       "value": value,
                                       "min_value": min_value,
                                       "max_value": max_value
                                   }
                                   consistency_metrics["violations"].append(violation)
                                   consistency_metrics["constraint_satisfaction"][constraint_id]["violations"].append(violation)

                   elif constraint_type == "dependency":
                       # Check if when one property exists, another required property also exists
                       source_property = constraint.get("source_property")
                       dependent_property = constraint.get("dependent_property")

                       if source_property in entity_data:
                           total_facts += 1

                           dependency_satisfied = dependent_property in entity_data
                           if dependency_satisfied:
                               consistent_facts += 1
                               consistency_metrics["constraint_satisfaction"][constraint_id]["total_satisfied"] += 1
                           else:
                               violation = {
                                   "entity_id": entity_id,
                                   "constraint_id": constraint_id,
                                   "source_property": source_property,
                                   "dependent_property": dependent_property
                               }
                               consistency_metrics["violations"].append(violation)
                               consistency_metrics["constraint_satisfaction"][constraint_id]["violations"].append(violation)

                   elif constraint_type == "custom":
                       # Execute custom constraint check
                       check_function = constraint.get("check_function")
                       if check_function and callable(check_function):
                           total_facts += 1

                           is_valid, details = check_function(entity_data)
                           if is_valid:
                               consistent_facts += 1
                               consistency_metrics["constraint_satisfaction"][constraint_id]["total_satisfied"] += 1
                           else:
                               violation = {
                                   "entity_id": entity_id,
                                   "constraint_id": constraint_id,
                                   "details": details
                               }
                               consistency_metrics["violations"].append(violation)
                               consistency_metrics["constraint_satisfaction"][constraint_id]["violations"].append(violation)

       # If ontology is provided, check for semantic consistency
       if ontology:
           # This would use an OWL reasoner or similar
           # Placeholder for ontology-based consistency checking
           pass

       # Calculate overall consistency ratio
       if total_facts > 0:
           consistency_metrics["overall_consistency"] = consistent_facts / total_facts

       # Calculate per-constraint satisfaction ratios
       for constraint_id, metrics in consistency_metrics["constraint_satisfaction"].items():
           if metrics["total_applicable"] > 0:
               metrics["satisfaction_ratio"] = metrics["total_satisfied"] / metrics["total_applicable"]
           else:
               metrics["satisfaction_ratio"] = 1.0  # No applicable entities

       return consistency_metrics

   def check_property_type(value, expected_type):
       """Check if a property value has the expected type"""
       if expected_type == "string":
           return isinstance(value, str)
       elif expected_type == "integer":
           return isinstance(value, int)
       elif expected_type == "float" or expected_type == "number":
           return isinstance(value, (int, float))
       elif expected_type == "boolean":
           return isinstance(value, bool)
       elif expected_type == "date":
           # Basic date format check
           if not isinstance(value, str):
               return False

           import re
           # Match YYYY-MM-DD format
           date_pattern = r'^\d{4}-\d{2}-\d{2}$'
           return bool(re.match(date_pattern, value))
       elif expected_type == "list":
           return isinstance(value, list)
       elif expected_type == "object":
           return isinstance(value, dict)
       elif expected_type == "uri":
           # Basic URI format check
           if not isinstance(value, str):
               return False

           import re
           uri_pattern = r'^[a-zA-Z][a-zA-Z0-9+.-]*:'
           return bool(re.match(uri_pattern, value))
       else:
           # Unknown type
           return True
   ```

:::

Challenges in quality assessment include:

1. **Scale and diversity**: Assessing quality across large, heterogeneous knowledge graphs
2. **Metric selection**: Identifying appropriate metrics for specific use cases
3. **Ground truth**: Establishing reference standards for accuracy evaluation
4. **Interdimensional trade-offs**: Balancing competing quality dimensions
5. **Resource constraints**: Managing the cost of quality assessment activities

### Knowledge graph completion

Knowledge graph completion identifies and fills in missing knowledge:

::: {#def-kg-completion}

## Knowledge graph completion

**Knowledge graph completion** is the process of identifying and filling in missing knowledge in a knowledge graph. Approaches include:

1. **Rule-based completion**: Using logical rules to infer missing facts
2. **Statistical completion**: Predicting missing links based on observed patterns
3. **Embedding-based completion**: Using vector representations to predict likely missing links
4. **External source completion**: Retrieving missing information from external knowledge sources
5. **Crowdsourced completion**: Engaging humans to provide missing knowledge

:::

::: {#exm-kg-completion}

## Knowledge graph completion examples

1. **Rule-based completion**:

   ```
   Rule: IF person1 has parent person2 AND person2 has parent person3 THEN person1 has grandparent person3

   Existing facts:
   - (John, hasParent, Mary)
   - (Mary, hasParent, Elizabeth)

   Inferred fact:
   - (John, hasGrandparent, Elizabeth)
   ```

2. **Embedding-based completion**:

   ```
   Entity embeddings:
   - France: [0.2, 0.8, -0.3, ...]
   - Germany: [0.3, 0.7, -0.2, ...]
   - Paris: [-0.1, 0.9, -0.4, ...]
   - Berlin: [0.0, 0.8, -0.3, ...]

   Relation embedding:
   - capitalOf: [0.5, -0.2, 0.1, ...]

   Known facts:
   - (Paris, capitalOf, France)

   Predicted fact (based on embedding similarity):
   - (Berlin, capitalOf, Germany)
   ```

3. **External source completion**:

   ```
   Knowledge graph entity:
   {
     "id": "entity:1234",
     "type": "Movie",
     "title": "The Matrix",
     "director": "The Wachowskis",
     "releaseYear": 1999
   }

   Query to external API:
   GET /movies?title=The+Matrix&year=1999

   External data:
   {
     "title": "The Matrix",
     "year": 1999,
     "runtime": 136,
     "genres": ["Action", "Sci-Fi"],
     "actors": ["Keanu Reeves", "Laurence Fishburne", "Carrie-Anne Moss"],
     "awards": "4 Oscars"
   }

   Completed entity:
   {
     "id": "entity:1234",
     "type": "Movie",
     "title": "The Matrix",
     "director": "The Wachowskis",
     "releaseYear": 1999,
     "runtime": 136,
     "genres": ["Action", "Sci-Fi"],
     "actors": ["Keanu Reeves", "Laurence Fishburne", "Carrie-Anne Moss"],
     "awards": "4 Oscars"
   }
   ```

4. **Statistical pattern-based completion**:

   ```
   Pattern: 87% of Technology companies have headquarters in one of [Silicon Valley, Seattle, Boston, New York]

   Entity with missing information:
   {
     "id": "company:5678",
     "type": "Company",
     "name": "TechCorp",
     "industry": "Technology",
     "foundedYear": 2010
   }

   Predicted completion (with confidence score):
   {
     "headquarters": "Silicon Valley",
     "confidence": 0.65
   }
   ```

:::

Knowledge graph completion approaches include:

1. **Logical inference**:

   - Deductive reasoning using ontological axioms
   - Rule-based systems for domain-specific patterns
   - Advantages: High precision, explainable results
   - Disadvantages: Limited to patterns expressible as rules

2. **Knowledge graph embeddings**:

   - Representing entities and relations in vector spaces
   - Models like TransE, DistMult, ComplEx, RotatE
   - Advantages: Can capture latent patterns, scalable
   - Disadvantages: Less interpretable, requires training data

3. **Graph neural networks**:

   - Neural networks operating directly on graph structures
   - Models like RGCN, CompGCN, KBGAT
   - Advantages: Captures complex structural patterns
   - Disadvantages: Computational complexity, training data requirements

4. **External knowledge integration**:
   - APIs, web scraping, database integration
   - Entity linking to existing knowledge bases
   - Advantages: Access to extensive external knowledge
   - Disadvantages: Integration challenges, source reliability

::: {#exm-kg-completion-implementation}

## Knowledge graph completion implementation

1. **Rule-based completion implementation**:

   ```python
   def rule_based_completion(knowledge_graph, rules):
       """
       Complete a knowledge graph using logical rules

       knowledge_graph: The knowledge graph to complete
       rules: List of rules for inferring new facts
       """
       # Convert knowledge graph to a more query-friendly format
       facts = []
       for entity_id, entity_data in knowledge_graph.items():
           for property_name, property_value in entity_data.items():
               if property_name in ["id", "type", "source"]:
                   continue

               # Handle both single values and lists
               if isinstance(property_value, list):
                   for value in property_value:
                       facts.append((entity_id, property_name, value))
               else:
                   facts.append((entity_id, property_name, property_value))

       # Apply rules until no new facts are derived
       new_facts_found = True
       inferred_facts = []

       while new_facts_found:
           new_facts_found = False

           for rule in rules:
               rule_type = rule.get("type")

               if rule_type == "transitive":
                   # Transitive rule: if (a, r, b) and (b, r, c) then (a, r, c)
                   relation = rule.get("relation")

                   # Find all facts with this relation
                   relation_facts = [(s, o) for s, r, o in facts if r == relation]

                   # Build a mapping for efficient lookup
                   relation_map = {}
                   for subject, obj in relation_facts:
                       if subject not in relation_map:
                           relation_map[subject] = []
                       relation_map[subject].append(obj)

                   # Apply transitive rule
                   for subject, objects in relation_map.items():
                       for obj in objects:
                           if obj in relation_map:
                               # Transitive step possible
                               for transitive_obj in relation_map[obj]:
                                   new_fact = (subject, relation, transitive_obj)

                                   # Check if this is a new fact
                                   if new_fact not in facts and new_fact not in inferred_facts:
                                       inferred_facts.append(new_fact)
                                       new_facts_found = True

               elif rule_type == "symmetric":
                   # Symmetric rule: if (a, r, b) then (b, r, a)
                   relation = rule.get("relation")

                   # Find all facts with this relation
                   relation_facts = [(s, o) for s, r, o in facts if r == relation]

                   # Apply symmetric rule
                   for subject, obj in relation_facts:
                       new_fact = (obj, relation, subject)

                       # Check if this is a new fact
                       if new_fact not in facts and new_fact not in inferred_facts:
                           inferred_facts.append(new_fact)
                           new_facts_found = True

               elif rule_type == "inverse":
                   # Inverse rule: if (a, r1, b) then (b, r2, a)
                   relation1 = rule.get("relation")
                   relation2 = rule.get("inverse_relation")

                   # Find all facts with relation1
                   relation_facts = [(s, o) for s, r, o in facts if r == relation1]

                   # Apply inverse rule
                   for subject, obj in relation_facts:
                       new_fact = (obj, relation2, subject)

                       # Check if this is a new fact
                       if new_fact not in facts and new_fact not in inferred_facts:
                           inferred_facts.append(new_fact)
                           new_facts_found = True

               elif rule_type == "composition":
                   # Composition rule: if (a, r1, b) and (b, r2, c) then (a, r3, c)
                   relation1 = rule.get("relation1")
                   relation2 = rule.get("relation2")
                   relation3 = rule.get("relation3")

                   # Find all facts with relation1 and relation2
                   relation1_facts = [(s, o) for s, r, o in facts if r == relation1]
                   relation2_facts = [(s, o) for s, r, o in facts if r == relation2]

                   # Build a mapping for relation2 for efficient lookup
                   relation2_map = {}
                   for subject, obj in relation2_facts:
                       if subject not in relation2_map:
                           relation2_map[subject] = []
                       relation2_map[subject].append(obj)

                   # Apply composition rule
                   for subject1, obj1 in relation1_facts:
                       if obj1 in relation2_map:
                           for obj2 in relation2_map[obj1]:
                               new_fact = (subject1, relation3, obj2)

                               # Check if this is a new fact
                               if new_fact not in facts and new_fact not in inferred_facts:
                                   inferred_facts.append(new_fact)
                                   new_facts_found = True

               elif rule_type == "custom":
                   # Custom rule with condition and conclusion functions
                   condition_func = rule.get("condition")
                   conclusion_func = rule.get("conclusion")

                   if callable(condition_func) and callable(conclusion_func):
                       # Apply custom rule
                       new_custom_facts = apply_custom_rule(
                           facts, condition_func, conclusion_func
                       )

                       for new_fact in new_custom_facts:
                           if new_fact not in facts and new_fact not in inferred_facts:
                               inferred_facts.append(new_fact)
                               new_facts_found = True

           # Add newly inferred facts to the existing facts
           facts.extend(inferred_facts)

       # Update the knowledge graph with inferred facts
       updated_knowledge_graph = knowledge_graph.copy()

       for subject, predicate, obj in inferred_facts:
           # Check if subject entity exists
           if subject not in updated_knowledge_graph:
               updated_knowledge_graph[subject] = {
                   "id": subject,
                   "inferred": True
               }

           # Add the new fact to the entity
           if predicate in updated_knowledge_graph[subject]:
               current_value = updated_knowledge_graph[subject][predicate]

               # Handle both single values and lists
               if isinstance(current_value, list):
                   if obj not in current_value:
                       updated_knowledge_graph[subject][predicate].append(obj)
               else:
                   if current_value != obj:
                       updated_knowledge_graph[subject][predicate] = [current_value, obj]
           else:
               updated_knowledge_graph[subject][predicate] = obj

       return updated_knowledge_graph, inferred_facts

   def apply_custom_rule(facts, condition_func, conclusion_func):
       """Apply a custom rule with condition and conclusion functions"""
       new_facts = []

       # Group facts by subject for efficient processing
       subject_facts = {}
       for subject, predicate, obj in facts:
           if subject not in subject_facts:
               subject_facts[subject] = []
           subject_facts[subject].append((predicate, obj))

       # Apply the rule to each entity
       for subject, entity_facts in subject_facts.items():
           # Check if condition matches
           if condition_func(subject, entity_facts, facts):
               # Generate new facts
               new_entity_facts = conclusion_func(subject, entity_facts, facts)
               new_facts.extend(new_entity_facts)

       return new_facts
   ```

2. **Embedding-based completion implementation**:

   ```python
   import numpy as np
   import torch
   import torch.nn as nn
   import torch.nn.functional as F
   import torch.optim as optim

   class TransE(nn.Module):
       """
       TransE knowledge graph embedding model

       TransE models relations as translations in the embedding space:
       h + r ≈ t for true triples (h, r, t)
       """
       def __init__(self, num_entities, num_relations, embedding_dim, margin=1.0):
           super(TransE, self).__init__()
           self.num_entities = num_entities
           self.num_relations = num_relations
           self.embedding_dim = embedding_dim
           self.margin = margin

           # Initialize entity and relation embeddings
           self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)
           self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)

           # Initialize weights
           nn.init.xavier_uniform_(self.entity_embeddings.weight)
           nn.init.xavier_uniform_(self.relation_embeddings.weight)

           # Normalize embeddings
           self.normalize_embeddings()

       def normalize_embeddings(self):
           """Normalize entity embeddings to unit L2 norm"""
           self.entity_embeddings.weight.data = F.normalize(
               self.entity_embeddings.weight.data, p=2, dim=1
           )

       def forward(self, positive_triples, negative_triples):
           """
           Compute the loss for a batch of positive and negative triples

           positive_triples: Tensor of shape (batch_size, 3) with positive triples (h, r, t)
           negative_triples: Tensor of shape (batch_size, 3) with negative triples (h', r, t')
           """
           # Extract positive triple components
           pos_heads = positive_triples[:, 0]
           pos_relations = positive_triples[:, 1]
           pos_tails = positive_triples[:, 2]

           # Extract negative triple components
           neg_heads = negative_triples[:, 0]
           neg_relations = negative_triples[:, 1]
           neg_tails = negative_triples[:, 2]

           # Get embeddings
           pos_head_embeddings = self.entity_embeddings(pos_heads)
           pos_relation_embeddings = self.relation_embeddings(pos_relations)
           pos_tail_embeddings = self.entity_embeddings(pos_tails)

           neg_head_embeddings = self.entity_embeddings(neg_heads)
           neg_relation_embeddings = self.relation_embeddings(neg_relations)
           neg_tail_embeddings = self.entity_embeddings(neg_tails)

           # Calculate scores
           pos_scores = torch.norm(
               pos_head_embeddings + pos_relation_embeddings - pos_tail_embeddings,
               p=1, dim=1
           )
           neg_scores = torch.norm(
               neg_head_embeddings + neg_relation_embeddings - neg_tail_embeddings,
               p=1, dim=1
           )

           # Calculate loss with margin
           loss = torch.mean(F.relu(self.margin + pos_scores - neg_scores))

           return loss

       def predict_tail(self, head, relation):
           """Predict the most likely tail entities for a given head and relation"""
           # Get embeddings
           head_embedding = self.entity_embeddings(torch.LongTensor([head]))
           relation_embedding = self.relation_embeddings(torch.LongTensor([relation]))

           # Calculate expected tail embedding
           expected_tail = head_embedding + relation_embedding

           # Calculate distance to all entity embeddings
           all_entities = torch.arange(self.num_entities)
           all_embeddings = self.entity_embeddings(all_entities)

           # Calculate scores (lower is better)
           scores = torch.norm(
               expected_tail.repeat(self.num_entities, 1) - all_embeddings,
               p=1, dim=1
           )

           # Return indices sorted by score (ascending)
           _, indices = torch.sort(scores)
           return indices.numpy()

       def predict_head(self, relation, tail):
           """Predict the most likely head entities for a given relation and tail"""
           # Get embeddings
           relation_embedding = self.relation_embeddings(torch.LongTensor([relation]))
           tail_embedding = self.entity_embeddings(torch.LongTensor([tail]))

           # Calculate expected head embedding
           expected_head = tail_embedding - relation_embedding

           # Calculate distance to all entity embeddings
           all_entities = torch.arange(self.num_entities)
           all_embeddings = self.entity_embeddings(all_entities)

           # Calculate scores (lower is better)
           scores = torch.norm(
               expected_head.repeat(self.num_entities, 1) - all_embeddings,
               p=1, dim=1
           )

           # Return indices sorted by score (ascending)
           _, indices = torch.sort(scores)
           return indices.numpy()

   def prepare_kg_data(knowledge_graph):
       """Prepare knowledge graph data for embedding model"""
       # Create entity and relation mappings
       entities = {}
       relations = {}

       triples = []

       # Extract triples from knowledge graph
       for entity_id, entity_data in knowledge_graph.items():
           # Add source entity to mapping if new
           if entity_id not in entities:
               entities[entity_id] = len(entities)

           # Process each property
           for prop, value in entity_data.items():
               if prop in ["id", "type", "source", "inferred"]:
                   continue

               # Add relation to mapping if new
               if prop not in relations:
                   relations[prop] = len(relations)

               # Handle both single values and lists
               if isinstance(value, list):
                   for v in value:
                       # Add target entity to mapping if new
                       if v not in entities:
                           entities[v] = len(entities)

                       # Add triple
                       triples.append((
                           entities[entity_id],
                           relations[prop],
                           entities[v]
                       ))
               else:
                   # Add target entity to mapping if new
                   if value not in entities:
                       entities[value] = len(entities)

                   # Add triple
                   triples.append((
                       entities[entity_id],
                       relations[prop],
                       entities[value]
                   ))

       # Create reverse mappings for looking up names
       id_to_entity = {idx: entity for entity, idx in entities.items()}
       id_to_relation = {idx: relation for relation, idx in relations.items()}

       return triples, entities, relations, id_to_entity, id_to_relation

   def train_kg_embeddings(knowledge_graph, embedding_dim=100, epochs=100, batch_size=128, margin=1.0, learning_rate=0.01):
       """Train knowledge graph embeddings and use them for completion"""
       # Prepare data
       triples, entity_map, relation_map, id_to_entity, id_to_relation = prepare_kg_data(
           knowledge_graph
       )

       # Convert triples to numpy array
       triples_array = np.array(triples)

       # Create model
       model = TransE(
           num_entities=len(entity_map),
           num_relations=len(relation_map),
           embedding_dim=embedding_dim,
           margin=margin
       )

       # Define optimizer
       optimizer = optim.Adam(model.parameters(), lr=learning_rate)

       # Training loop
       for epoch in range(epochs):
           # Shuffle triples
           np.random.shuffle(triples_array)

           total_loss = 0
           num_batches = len(triples_array) // batch_size + (1 if len(triples_array) % batch_size != 0 else 0)

           for i in range(0, len(triples_array), batch_size):
               # Get batch
               batch = triples_array[i:i+batch_size]

               # Generate negative samples (random corruption of head or tail)
               negative_batch = []
               for h, r, t in batch:
                   if np.random.random() < 0.5:
                       # Corrupt head
                       h_neg = h
                       while h_neg == h:
                           h_neg = np.random.randint(0, len(entity_map))
                       negative_batch.append((h_neg, r, t))
                   else:
                       # Corrupt tail
                       t_neg = t
                       while t_neg == t:
                           t_neg = np.random.randint(0, len(entity_map))
                       negative_batch.append((h, r, t_neg))

               # Convert to tensors
               positive_triples = torch.LongTensor(batch)
               negative_triples = torch.LongTensor(negative_batch)

               # Forward and backward passes
               optimizer.zero_grad()
               loss = model(positive_triples, negative_triples)
               loss.backward()
               optimizer.step()

               # Normalize embeddings after update
               model.normalize_embeddings()

               total_loss += loss.item()

           # Print progress
           if (epoch + 1) % 10 == 0:
               average_loss = total_loss / num_batches
               print(f"Epoch {epoch+1}/{epochs}, Loss: {average_loss:.4f}")

       return model, entity_map, relation_map, id_to_entity, id_to_relation

   def complete_kg_with_embeddings(knowledge_graph, top_k=5, threshold=0.8):
       """Complete a knowledge graph using embedding-based predictions"""
       # Train embeddings
       model, entity_map, relation_map, id_to_entity, id_to_relation = train_kg_embeddings(
           knowledge_graph
       )

       # Prepare for completion
       completed_kg = knowledge_graph.copy()
       new_facts = []

       # For each entity-relation pair, predict potential missing tails
       for entity_id, entity_data in knowledge_graph.items():
           entity_idx = entity_map[entity_id]

           # Check each relation
           for relation, rel_idx in relation_map.items():
               # Skip if this relation already exists for this entity
               if relation in entity_data:
                   continue

               # Predict tails
               predicted_tails = model.predict_tail(entity_idx, rel_idx)
               top_predictions = predicted_tails[:top_k]

               # Convert predictions to entities and add to completed graph
               for tail_idx in top_predictions:
                   tail_id = id_to_entity[tail_idx.item()]
                   confidence = 1.0 - (tail_idx.item() / len(entity_map))  # Simple confidence heuristic

                   if confidence >= threshold:
                       new_fact = {
                           "subject": entity_id,
                           "predicate": relation,
                           "object": tail_id,
                           "confidence": confidence,
                           "method": "embedding"
                       }
                       new_facts.append(new_fact)

                       # Add to completed KG
                       if relation not in completed_kg[entity_id]:
                           completed_kg[entity_id][relation] = tail_id
                       elif isinstance(completed_kg[entity_id][relation], list):
                           completed_kg[entity_id][relation].append(tail_id)
                       else:
                           completed_kg[entity_id][relation] = [completed_kg[entity_id][relation], tail_id]

       return completed_kg, new_facts, model
   ```

3. **External source completion implementation**:

   ```python
   import requests
   import json
   from concurrent.futures import ThreadPoolExecutor

   def complete_from_external_sources(knowledge_graph, source_configs, max_workers=5):
       """
       Complete a knowledge graph using external data sources

       knowledge_graph: The knowledge graph to complete
       source_configs: Configurations for external data sources
       max_workers: Maximum number of concurrent workers for API calls
       """
       completed_kg = knowledge_graph.copy()
       completion_stats = {
           "entities_processed": 0,
           "entities_enriched": 0,
           "new_facts_added": 0,
           "by_source": {},
           "by_property": {}
       }

       # Initialize stats
       for source in source_configs:
           source_id = source.get("id")
           completion_stats["by_source"][source_id] = {
               "entities_processed": 0,
               "entities_enriched": 0,
               "new_facts_added": 0
           }

       # Process entities in parallel
       with ThreadPoolExecutor(max_workers=max_workers) as executor:
           # Create tasks for each entity
           future_to_entity = {
               executor.submit(process_entity, entity_id, entity_data, source_configs):
               (entity_id, entity_data)
               for entity_id, entity_data in knowledge_graph.items()
           }

           # Process results as they complete
           for future in future_to_entity:
               entity_id, entity_data = future_to_entity[future]

               try:
                   enrichment_result = future.result()

                   # Update the knowledge graph with enriched data
                   if enrichment_result and enrichment_result.get("enriched_data"):
                       completion_stats["entities_processed"] += 1

                       enriched_entity = enrichment_result["enriched_data"]
                       entity_enriched = False

                       # Update entity properties
                       for prop, value in enriched_entity.items():
                           # Skip if property already exists
                           if prop in entity_data and entity_data[prop] == value:
                               continue

                           # Add new property or extend existing one
                           if prop not in entity_data:
                               completed_kg[entity_id][prop] = value
                               entity_enriched = True
                               completion_stats["new_facts_added"] += 1

                               # Track by property
                               if prop not in completion_stats["by_property"]:
                                   completion_stats["by_property"][prop] = 0
                               completion_stats["by_property"][prop] += 1
                           elif isinstance(entity_data[prop], list):
                               # If current value is a list, append new value if it's not already included
                               if value not in entity_data[prop]:
                                   if isinstance(value, list):
                                       # Extend with new values
                                       for v in value:
                                           if v not in completed_kg[entity_id][prop]:
                                               completed_kg[entity_id][prop].append(v)
                                               entity_enriched = True
                                               completion_stats["new_facts_added"] += 1

                                               if prop not in completion_stats["by_property"]:
                                                   completion_stats["by_property"][prop] = 0
                                               completion_stats["by_property"][prop] += 1
                                   else:
                                       # Append single value
                                       completed_kg[entity_id][prop].append(value)
                                       entity_enriched = True
                                       completion_stats["new_facts_added"] += 1

                                       if prop not in completion_stats["by_property"]:
                                           completion_stats["by_property"][prop] = 0
                                       completion_stats["by_property"][prop] += 1
                           else:
                               # Current value is singular - convert to list
                               completed_kg[entity_id][prop] = [entity_data[prop], value]
                               entity_enriched = True
                               completion_stats["new_facts_added"] += 1

                               if prop not in completion_stats["by_property"]:
                                   completion_stats["by_property"][prop] = 0
                               completion_stats["by_property"][prop] += 1

                       # Update source statistics
                       for source_id, source_stats in enrichment_result["stats"].items():
                           completion_stats["by_source"][source_id]["entities_processed"] += 1

                           if source_stats["facts_added"] > 0:
                               completion_stats["by_source"][source_id]["entities_enriched"] += 1
                               completion_stats["by_source"][source_id]["new_facts_added"] += source_stats["facts_added"]

                       if entity_enriched:
                           completion_stats["entities_enriched"] += 1

                           # Add provenance information
                           if "sources" not in completed_kg[entity_id]:
                               completed_kg[entity_id]["sources"] = []

                           for source_id in enrichment_result["stats"]:
                               if source_id not in completed_kg[entity_id]["sources"]:
                                   completed_kg[entity_id]["sources"].append(source_id)

               except Exception as e:
                   print(f"Error processing entity {entity_id}: {str(e)}")

       return completed_kg, completion_stats

   def process_entity(entity_id, entity_data, source_configs):
       """Process an entity through all configured external sources"""
       entity_type = entity_data.get("type")
       enriched_data = {}
       stats = {}

       for source_config in source_configs:
           source_id = source_config.get("id")
           applicable_types = source_config.get("applicable_types", [])

           # Skip if this source is not applicable for this entity type
           if applicable_types and entity_type not in applicable_types:
               continue

           # Try to enrich with this source
           try:
               source_result = query_external_source(
                   entity_id, entity_data, source_config
               )

               if source_result:
                   # Merge with existing enriched data
                   for prop, value in source_result.items():
                       enriched_data[prop] = value

                   # Track statistics
                   stats[source_id] = {
                       "facts_added": len(source_result)
                   }
           except Exception as e:
               print(f"Error querying source {source_id} for entity {entity_id}: {str(e)}")
               stats[source_id] = {
                       "facts_added": 0
               }

       return {
           "entity_id": entity_id,
           "enriched_data": enriched_data,
           "stats": stats
       }

   def query_external_source(entity_id, entity_data, source_config):
       """Query an external source for additional entity information"""
       source_type = source_config.get("type")

       if source_type == "rest_api":
           return query_rest_api(entity_id, entity_data, source_config)
       elif source_type == "sparql_endpoint":
           return query_sparql_endpoint(entity_id, entity_data, source_config)
       elif source_type == "custom":
           custom_function = source_config.get("query_function")
           if callable(custom_function):
               return custom_function(entity_id, entity_data, source_config)

       return None

   def query_rest_api(entity_id, entity_data, source_config):
       """Query a REST API for entity enrichment"""
       base_url = source_config.get("base_url")
       endpoint = source_config.get("endpoint")
       method = source_config.get("method", "GET")
       headers = source_config.get("headers", {})

       # Prepare query parameters
       params = {}
       param_mapping = source_config.get("param_mapping", {})

       for param_name, entity_prop in param_mapping.items():
           if entity_prop in entity_data:
               params[param_name] = entity_data[entity_prop]

       # Add API key if provided
       api_key = source_config.get("api_key")
       api_key_param = source_config.get("api_key_param")

       if api_key and api_key_param:
           params[api_key_param] = api_key

       # Make the request
       url = f"{base_url}/{endpoint}"

       if method == "GET":
           response = requests.get(url, params=params, headers=headers)
       elif method == "POST":
           response = requests.post(url, json=params, headers=headers)
       else:
           raise ValueError(f"Unsupported method: {method}")

       # Process response
       if response.status_code == 200:
           data = response.json()

           # Process the response data
           result = {}

           # Apply response mapping
           response_mapping = source_config.get("response_mapping", {})

           for kg_prop, response_path in response_mapping.items():
               value = extract_from_json_path(data, response_path)
               if value is not None:
                   result[kg_prop] = value

           return result
       else:
           print(f"API request failed with status code {response.status_code}: {response.text}")
           return None

   def extract_from_json_path(data, path):
       """Extract a value from a nested JSON structure using a dot-notation path"""
       if not path:
           return data

       parts = path.split(".")
       current = data

       for part in parts:
           # Handle array indexing
           if "[" in part and part.endswith("]"):
               array_part, idx_part = part.split("[", 1)
               idx = int(idx_part[:-1])  # Remove closing bracket and convert to int

               if array_part:
                   if array_part not in current:
                       return None
                   current = current[array_part]

               if isinstance(current, list) and 0 <= idx < len(current):
                   current = current[idx]
               else:
                   return None
           else:
               # Regular property access
               if isinstance(current, dict) and part in current:
                   current = current[part]
               else:
                   return None

       return current

   def query_sparql_endpoint(entity_id, entity_data, source_config):
       """Query a SPARQL endpoint for entity enrichment"""
       # This implementation would use the SPARQLWrapper library or similar
       # to query a SPARQL endpoint for additional entity information

       # Simplified placeholder implementation
       sparql_endpoint = source_config.get("endpoint_url")
       query_template = source_config.get("query_template")

       # Create query from template
       query = query_template
       for placeholder, prop in source_config.get("query_mapping", {}).items():
           if prop in entity_data:
               value = entity_data[prop]

               # Format value based on type
               if isinstance(value, str):
                   formatted_value = f'"{value}"'
               else:
                   formatted_value = str(value)

               query = query.replace(placeholder, formatted_value)

       # Execute query and process results
       # In a real implementation, you would use SPARQLWrapper or similar

       # Placeholder - return empty result
       return {}
   ```

:::

Challenges in knowledge graph completion include:

1. **Balancing precision and recall**: Determining the confidence threshold for accepting predictions
2. **Handling conflicting completions**: Resolving contradictions from different completion methods
3. **Evaluating completion quality**: Assessing the accuracy of automatically derived knowledge
4. **Domain-specific knowledge**: Adapting completion approaches to specific domains
5. **Computational complexity**: Managing the computational cost of completion algorithms

### Context and provenance enrichment

Enriching knowledge graphs with context and provenance information enhances their trustworthiness and utility:

::: {#def-provenance}

## Context and provenance enrichment

**Context enrichment** adds situational information to knowledge graph facts, such as:

1. Temporal context (when a fact is valid)
2. Spatial context (where a fact applies)
3. Conditional context (under what conditions a fact holds)

**Provenance enrichment** adds metadata about the origin and derivation of knowledge, including:

1. Source attribution (where information came from)
2. Extraction method (how information was obtained)
3. Confidence scores (how certain the information is)
4. Derivation chains (how inferred knowledge was derived)

:::

::: {#exm-provenance}

## Context and provenance enrichment examples

1. **Temporal context**:

   ```
   Triple: (Berlin, capitalOf, Germany)

   Enriched with temporal context:
   {
     "fact": "Berlin is the capital of Germany",
     "subject": "Berlin",
     "predicate": "capitalOf",
     "object": "Germany",
     "validFrom": "1990-10-03",
     "validUntil": null
   }
   ```

2. **Source provenance**:

   ```
   Triple: (Aspirin, treats, Headache)

   Enriched with source provenance:
   {
     "fact": "Aspirin treats headaches",
     "subject": "Aspirin",
     "predicate": "treats",
     "object": "Headache",
     "sources": [
       {
         "id": "PMID:12345678",
         "type": "medical_literature",
         "title": "Efficacy of aspirin for tension headaches",
         "year": 2015,
         "reliability": 0.9
       },
       {
         "id": "DrugBank:DB00945",
         "type": "pharmaceutical_database",
         "version": "5.1.8",
         "reliability": 0.95
       }
     ]
   }
   ```

3. **Extraction provenance**:

   ```
   Triple: (COVID-19, causedBy, SARS-CoV-2)

   Enriched with extraction provenance:
   {
     "fact": "COVID-19 is caused by SARS-CoV-2",
     "subject": "COVID-19",
     "predicate": "causedBy",
     "object": "SARS-CoV-2",
     "extraction": {
       "method": "NLP",
       "tool": "BioBERT v1.1",
       "source_text": "The novel coronavirus SARS-CoV-2 is the causative agent of COVID-19.",
       "confidence": 0.96,
       "extracted_date": "2023-05-10T14:32:17Z"
     }
   }
   ```

4. **Derivation provenance**:

   ```
   Triple: (Alice, grandparentOf, Charlie)

   Enriched with derivation provenance:
   {
     "fact": "Alice is a grandparent of Charlie",
     "subject": "Alice",
     "predicate": "grandparentOf",
     "object": "Charlie",
     "derivation": {
       "type": "inference",
       "rule": "IF X parentOf Y AND Y parentOf Z THEN X grandparentOf Z",
       "premises": [
         {
           "subject": "Alice",
           "predicate": "parentOf",
           "object": "Bob",
           "source": "Birth Certificate DB"
         },
         {
           "subject": "Bob",
           "predicate": "parentOf",
           "object": "Charlie",
           "source": "School Registration DB"
         }
       ],
       "confidence": 0.99
     }
   }
   ```

:::

Approaches to context and provenance enrichment include:

1. **Metadata models**: Standards and schemas for representing provenance

   - PROV-O (Provenance Ontology)
   - Dublin Core
   - W3C Web Annotation Data Model
   - Named graphs and RDF quads (for context management)

2. **Confidence scoring mechanisms**: Methods for quantifying certainty

   - Probabilistic models
   - Fuzzy truth values
   - Evidence-based scoring
   - Source reliability weighting

3. **Temporal modeling**: Representing time-varying knowledge
   - Time interval representation
   - Versioning systems
   - Temporal query languages
   - Validity periods

Challenges in provenance enrichment include:

1. **Granularity trade-offs**: Determining the appropriate level of provenance detail
2. **Computational overhead**: Managing the increased storage and processing requirements
3. **Provenance integration**: Combining provenance from diverse sources
4. **Query complexity**: Supporting provenance-aware queries efficiently
5. **User interface design**: Presenting provenance information effectively to users

### Semantic enrichment and alignment

Semantic enrichment enhances knowledge graphs with deeper meaning and connections to established semantic resources:

::: {#def-semantic-enrichment}

## Semantic enrichment and alignment

**Semantic enrichment** adds deeper semantic context to knowledge graph elements through:

1. Type enhancement and ontological classification
2. Semantic annotation with domain concepts
3. Linking to semantic resources and lexical databases
4. Relationship qualification and contextualization

**Semantic alignment** connects knowledge graph elements to established semantic resources like:

1. WordNet, ConceptNet, or BabelNet for lexical relations
2. Domain-specific ontologies for specialized terminology
3. Upper ontologies for fundamental categories
4. Linked Open Data resources for global entity alignment

:::

::: {#exm-semantic-enrichment}

## Semantic enrichment examples

1. **Type enhancement**:

   ```
   Original entity:
   {
     "id": "entity:12345",
     "type": "Person",
     "name": "Marie Curie",
     "field": "Physics"
   }

   Semantically enriched:
   {
     "id": "entity:12345",
     "type": ["Person", "Scientist", "Physicist", "NobelLaureate", "Researcher"],
     "name": "Marie Curie",
     "field": "Physics",
     "typeHierarchy": {
       "Person": {
         "Scientist": {
           "Physicist": {}
         },
         "Researcher": {}
       },
       "NobelLaureate": {}
     }
   }
   ```

2. **Lexical enrichment**:

   ```
   Original relation:
   (Cancer, affects, Lung)

   Semantically enriched with WordNet:
   {
     "relation": "affects",
     "wordnet": {
       "synset": "affect.v.01",
       "definition": "have an effect upon",
       "synonyms": ["influence", "impact", "alter"],
       "hypernyms": ["change.v.01"]
     }
   }
   ```

3. **Semantic web alignment**:

   ```
   Original entity:
   {
     "id": "location:paris",
     "type": "City",
     "name": "Paris",
     "country": "France"
   }

   With semantic web alignment:
   {
     "id": "location:paris",
     "type": "City",
     "name": "Paris",
     "country": "France",
     "sameAs": [
       "http://dbpedia.org/resource/Paris",
       "http://www.wikidata.org/entity/Q90",
       "http://sws.geonames.org/2988507/"
     ],
     "schema:type": "http://schema.org/City"
   }
   ```

4. **Conceptual enrichment**:

   ```
   Original concept:
   {
     "id": "concept:machine_learning",
     "type": "Topic",
     "name": "Machine Learning"
   }

   With conceptual enrichment:
   {
     "id": "concept:machine_learning",
     "type": "Topic",
     "name": "Machine Learning",
     "broader": ["Artificial Intelligence", "Computer Science"],
     "narrower": ["Neural Networks", "Decision Trees", "Reinforcement Learning"],
     "related": ["Data Mining", "Pattern Recognition", "Statistical Analysis"],
     "definition": "Field of study that gives computers the ability to learn without being explicitly programmed"
   }
   ```

:::

Implementation approaches for semantic enrichment include:

1. **Entity linking**: Connecting mentions to knowledge bases

   - Example: Linking "NYC" to DBpedia entity for New York City
   - Techniques: Candidate generation, context-based disambiguation, graph-based alignment

2. **Type inference**: Determining more specific entity types

   - Example: Inferring that an entity is not just a Person but specifically a Scientist
   - Techniques: Pattern-based inference, statistical classification, ontological reasoning

3. **Relationship qualification**: Enriching relationship semantics

   - Example: Adding temporal scope, certainty, or directness to relationships
   - Techniques: Pattern analysis, contextual inference, semantic frames

4. **Cross-lingual alignment**: Connecting across language barriers
   - Example: Aligning "Maladie" (French) with "Disease" (English)
   - Techniques: Multilingual embeddings, translation-based alignment, shared identifiers

Challenges in semantic enrichment include:

1. **Semantic ambiguity**: Resolving multiple possible meanings
2. **Knowledge base coverage**: Handling entities not present in reference resources
3. **Cross-domain alignment**: Connecting concepts across different domains
4. **Semantic drift**: Managing meaning changes over time or context
5. **Integration complexity**: Combining multiple semantic resources coherently

## Summary

This chapter has explored the process of knowledge graph construction, covering the entire lifecycle from data extraction to quality assessment and enrichment. We've examined various approaches for extracting knowledge from structured, semi-structured, and unstructured data sources, and techniques for integrating this knowledge into a coherent and high-quality graph.

Key topics covered include:

1. **Knowledge graph construction lifecycle**: The systematic process of planning, extracting, integrating, and maintaining knowledge graphs, with different architectural approaches for implementation.

2. **Data sources for knowledge graphs**: The variety of sources that can contribute to knowledge graphs, including structured databases, semi-structured web content, and unstructured text, each requiring specialized extraction techniques.

3. **Information extraction techniques**: Methods for identifying entities, relationships, attributes, and events from various data sources, including specialized approaches for domain-specific knowledge.

4. **Entity resolution and linking**: Techniques for identifying when different mentions refer to the same entity and connecting extracted entities to existing knowledge sources, critical for building coherent knowledge graphs.

5. **Knowledge graph integration and fusion**: Approaches for combining information from multiple sources while addressing schema heterogeneity, resolving conflicts, and ensuring consistency.

6. **Quality assessment and enrichment**: Methods for evaluating knowledge graph quality along multiple dimensions and enhancing graphs with additional knowledge, context, and semantic connections.

The construction of high-quality knowledge graphs remains a complex challenge, requiring a combination of automated techniques and human expertise. However, the methodologies and tools discussed in this chapter provide a framework for systematically addressing this challenge, enabling the creation of knowledge graphs that can serve as valuable resources for a wide range of applications.

As knowledge graph technology continues to evolve, construction approaches are becoming more sophisticated, leveraging advances in machine learning, natural language processing, and semantic technologies to automate and improve the extraction and integration of knowledge from diverse sources. At the same time, the importance of quality assessment, provenance tracking, and semantic enrichment is increasingly recognized as critical for ensuring that knowledge graphs can be trusted and effectively utilized in real-world applications.

## Exercises

::: {#exr-construction-lifecycle}

## Knowledge graph construction lifecycle

Design a complete construction lifecycle for a knowledge graph in a domain of your choice (e.g., healthcare, finance, scientific research). Your design should include:

1. Identify key data sources (at least one of each type: structured, semi-structured, unstructured)
2. Specify the extraction approach for each source
3. Describe the integration strategy, including schema alignment
4. Outline a quality assessment plan with specific metrics
5. Propose an enrichment strategy to enhance the initial graph

Discuss potential challenges in each phase and how you would address them.

:::

::: {#exr-information-extraction}

## Information extraction techniques

Compare and contrast three different approaches to relationship extraction:

1. Pattern-based extraction
2. Supervised machine learning
3. Distant supervision

For each approach:

- Explain how it works conceptually
- Identify its strengths and limitations
- Describe a scenario where it would be the most appropriate choice
- Provide an example of how it would extract a specific relationship from text

Conclude with guidelines for selecting an appropriate extraction approach based on data characteristics and project requirements.

:::

::: {#exr-entity-resolution}

## Entity resolution scenario

You are building a knowledge graph about scientific publications by integrating data from:

1. A bibliographic database with paper metadata
2. A university database with researcher information
3. A conference proceedings database
4. A scientific social network

The same researchers and papers may appear in multiple sources with variations in names, affiliations, and other attributes.

Design an entity resolution pipeline that addresses:

1. Researcher identity resolution across sources
2. Paper deduplication and alignment
3. Affiliation normalization and matching
4. Handling of temporal changes (e.g., name changes, changing affiliations)

Include both technical approaches and practical considerations in your design.

:::

::: {#exr-quality-assessment}

## Knowledge graph quality assessment

Develop a comprehensive quality assessment framework for a knowledge graph, addressing:

1. Define at least three metrics for each of the following dimensions:

   - Accuracy
   - Completeness
   - Consistency
   - Timeliness

2. For each metric:

   - Provide a formal definition
   - Describe how it would be measured in practice
   - Identify acceptable threshold values
   - Explain how to interpret the results

3. Design a visualization approach that would effectively communicate quality assessment results to stakeholders.

4. Propose a remediation strategy for addressing quality issues identified through your assessment.

:::

::: {#exr-knowledge-completion}

## Knowledge graph completion techniques

For a knowledge graph about movies, directors, actors, and genres:

1. Design a rule-based completion approach that would infer at least three types of missing relationships
2. Outline how a knowledge graph embedding model would help predict missing links
3. Describe an external source integration strategy to add missing information
4. Develop a hybrid approach that combines the strengths of multiple completion methods

Evaluate the trade-offs of each approach in terms of precision, recall, scalability, and explainability.

:::

## Further reading

1. Paulheim, H. (2017). Knowledge Graph Refinement: A Survey of Approaches and Evaluation Methods. Semantic Web, 8(3), 489-508.

2. Weikum, G., Dong, X. L., Razniewski, S., & Suchanek, F. M. (2020). Machine Knowledge: Creation and Curation of Comprehensive Knowledge Bases. Foundations and Trends in Databases, 10(2-4), 108-490.

3. Shen, W., Wang, J., & Han, J. (2015). Entity Linking with a Knowledge Base: Issues, Techniques, and Solutions. IEEE Transactions on Knowledge and Data Engineering, 27(2), 443-460.

4. Christen, P. (2012). Data Matching: Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection. Springer.

5. Ji, S., Pan, S., Cambria, E., Marttinen, P., & Yu, P. S. (2021). A Survey on Knowledge Graphs: Representation, Acquisition, and Applications. IEEE Transactions on Neural Networks and Learning Systems.

6. Farid, D. M., Nowe, A., & Hasan, M. A. (2020). A Survey of Knowledge Graph Completion Approaches: Methods, Applications, and Challenges. Proceedings of the Future Technologies Conference, 766-793.

7. Zaveri, A., Rula, A., Maurino, A., Pietrobon, R., Lehmann, J., & Auer, S. (2016). Quality Assessment for Linked Data: A Survey. Semantic Web, 7(1), 63-93.

8. Nickel, M., Murphy, K., Tresp, V., & Gabrilovich, E. (2016). A Review of Relational Machine Learning for Knowledge Graphs. Proceedings of the IEEE, 104(1), 11-33.

9. Soru, T., Marx, E., Moussallem, D., Publio, G., Valdestilhas, A., Esteves, D., & Neto, C. B. (2017). SPARQL as a Foreign Language. arXiv preprint arXiv:1708.07624.

10. Ilievski, F., Szekely, P., & Zhang, B. (2021). Onto-Cognitive Scaffolding: Developing Methods for Placing Knowledge Graphs on a Cognitive Leash. arXiv preprint arXiv:2105.05757.
